---
title: |
  | Homework 3: Spark (RDDs, DF) II
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
linkcolor: blue
subtitle: |
  | E-63 Big Data Analytics
  | Harvard University, Autumn 2017
affiliation: Harvard University
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```

## Problem 1 (30%)
Consider two attached text files: *bible.txt* and *4300.txt*. The first contains ASCII text of King James Bible and the other the text of James Joyce’s novel Ulysses.

### i) RDD word number pairs
Use Spark transformation and action functions present in *RDD API* to transform those texts into RDD-s that contain words and numbers of occurrence of those words in respective text.

#### Install findspark
We're going to use the standard python 2.7 interpreter to solve the above problem. In order to do this we're first installing findspark with pip

```{bash eval=FALSE} 
sudo -H pip install findspark 
```

#### Cleanup function
```{python eval=FALSE}
# Import regex library
import re

# Cleanup function
def clean_up(rdd_words):
	rdd_words_clean = re.sub(r'([^A-Za-z0-9\s+])', '', rdd_words)  
	rdd_words_split = rdd_words_clean.split(' ')
	return [word.lower() for word in rdd_words_split if word != '']
```


```{python eval=FALSE}
# Import libraries
import findspark
findspark.init("/opt/spark-2.2.0-bin-hadoop2.7")
from pyspark import SparkContext, SparkConf

# Start session
conf = SparkConf().setMaster("local").setAppName("p1_i_rdd")
sc = SparkContext(conf = conf)

# Read data
rdd_ulysses = sc.textFile("/home/tim/e63-coursework/hw4/data/4300-2.txt")
rdd_bible = sc.textFile("/home/tim/e63-coursework/hw4/data/bible.txt")

# Ulysses: Mapreduce
rdd_ulysess = rdd_ulysses.flatMap(clean_up)
rdd_ulysess_all = rdd_ulysess.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)

rdd_ulysess_all.take(5)

# Bible: Mapreduce
rdd_bible = rdd_bible.flatMap(clean_up)
rdd_bible_all = rdd_bible.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)

rdd_bible_all.take(5)

sc.stop()
```

```{bash echo=FALSE}
# Execute python script
/opt/spark-2.2.0-bin-hadoop2.7/bin/spark-submit \
  /mnt/c/Local/Education/e63-coursework/hw3/scripts/p3_rdd_lambda_count.py
```

### ii) Eliminate stop words
From King James Bible eliminate all verse numbers of the form: 03:019:024. Eliminate from both RDDs so called *“stop words”*. Please use the list of stop words on Web page: http://www.lextek.com/manuals/onix/stopwords1.html.

#### Download Stopwords
```{bash eval=FALSE}
wget http://www.lextek.com/manuals/onix/stopwords1.html
```

### iii) Get unique words
Create RDD-s that contain only words unique for each of text.

### iv) Get common words
Finally create an RDD that contains only the words common to both texts. In latest RDD preserve numbers of occurrences in two texts. In other words a row in your RDD will look like (love 45 32). List for us 30 most frequent words in each RDD (text). Print or store the words and the numbers of occurrences.

### v) 20 most frequent words
Create for us the list of 20 most frequently used words common to both texts. In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible. Order your report in descending order starting by the number of occurrences in Ulysses. Present the same data this time ordered by the number of occurrences in the Bible.

### vi) Get a random sample
List for us a random samples containing 5% of words in the final RDD.


## Problem 2 (20%)
Implement problem 1 using DataFrame API.


## Problem 3 (30%)
Consider attached files *transactions.txt* and *products.txt*.

### i) Load data
Each line in *transactions.txt* file contains a *transaction date*, *time*, *customer id*, *product id*, *quantity bought* and *price paid*, delimited with hash (#) sign. Each line in file *products.txt* contains *product id*, *product name*, *unit price* and *quantity available* in the store. Bring those data in Spark and organize it as DataFrames with named columns.

### ii) Largest spending
Using either DataFrame methods or plain SQL statements find 5 customers with the largest spent on the day. Find the names of the products each of those 5 customers bought.

### iii) Total number sold
Find the names and total number sold of 10 most popular products. Order products once per the number sold and then by the total value (quanity*price) sold. 

## Problem 4 (20%)
Implement problem 3 using RDD APIs.
