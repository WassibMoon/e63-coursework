---
title: |
  | Homework 4: Spark (RDDs, DF) II
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
linkcolor: blue
subtitle: |
  | E-63 Big Data Analytics
  | Harvard University, Autumn 2017
affiliation: Harvard University
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```

## Problem 1 (30%)
Consider two attached text files: *bible.txt* and *4300.txt*. The first contains ASCII text of King James Bible and the other the text of James Joyce’s novel Ulysses.

### i) Download stop words
Download and parse a list of *stop words* from the web page: http://www.lextek.com/manuals/onix/stopwords1.html.

#### Download Stopwords
```{python eval=FALSE}
# Load libraries
import requests
import csv
from bs4 import BeautifulSoup

# Download page
page = requests.get("http://www.lextek.com/manuals/onix/stopwords1.html")

# Parse page
html = BeautifulSoup(page.content, 'html.parser').pre
text = html.get_text().split()

# Remove introduction
stopwords = text[21:len(text)]

## Export data to a datafile
result_file = open("/home/tim/e63-coursework/hw4/data/stopwords.csv", 'w')
for i in stopwords:
  result_file.write(i + "\n")

result_file.close
```

### ii) RDD word number pairs
Use Spark transformation and action functions present in *RDD API* to transform those texts into RDD-s that contain words and numbers of occurrence of those words in respective text. From King James Bible eliminate all verse numbers of the form: *03:019:024*. Eliminate from both RDDs so called *“stop words”*. List for us 30 most frequent words in each RDD (text).

#### Cleanup function
```{python eval=FALSE}
# Cleanup function
def clean_up(rdd_words):
  import re # Import regex library
  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse
  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\s+])', '', rdd_words_clean1) # Nonwords  
  rdd_words_split = rdd_words_clean2.split(' ') # Split data
  return [word.lower() for word in rdd_words_split if word != ''] # Lower case

```

#### Load data into RDD and cleanup
```{python eval=FALSE}
# Import libraries
import findspark
findspark.init("/opt/spark-2.2.0-bin-hadoop2.7")
from pyspark import SparkContext, SparkConf

# Start session
conf = SparkConf().setMaster("local").setAppName("p1_rdd")
sc = SparkContext(conf = conf)

# Read data
rdd_ulysses = sc.textFile("/home/tim/e63-coursework/hw4/data/4300-2.txt")
rdd_bible = sc.textFile("/home/tim/e63-coursework/hw4/data/bible.txt")
rdd_stopwords = sc.textFile("/home/tim/e63-coursework/hw4/data/stopwords.csv")

# Clean data and remove stopwords and verse number
rdd_ulysess = rdd_ulysses.flatMap(clean_up)
rdd_ulysess_cleaned = rdd_ulysess.subtract(rdd_stopwords)

rdd_bible = rdd_bible.flatMap(clean_up)
rdd_bible_cleaned = rdd_bible.subtract(rdd_stopwords)

# Number of occurence (Mapreduce)
rdd_ulysess_all = rdd_ulysess_cleaned.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
rdd_bible_all = rdd_bible_cleaned.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)

# Print five word-pairs
print "Ulysess - Top 30 word pairs:"
print rdd_ulysess_all.map(lambda (k,v): (v,k)).sortByKey(False).take(30)

print "Bible - Top 30 word pairs:"
print rdd_bible_all.map(lambda (k,v): (v,k)).sortByKey(False).take(30)
```

### iii) Get unique words
Create RDD-s that contain only words unique for each of text.

```{python eval=FALSE}
rdd_ulysess_dist = rdd_ulysess_cleaned.distinct()
rdd_bible_dist = rdd_bible_cleaned.distinct()

# Number of unique words
rdd_ulysess_dist.count()
rdd_bible_dist.count()
```

### iv) Get common words
Finally create an RDD that contains only the words common to both texts. In latest RDD preserve numbers of occurrences in two texts. In other words a row in your RDD will look like (love 45 32). Print or store the words and the numbers of occurrences.

```{python eval=FALSE}
#common_words = rdd_ulysess_cleaned.intersection(rdd_bible_cleaned)
rdd_combined = rdd_ulysess_all.join(rdd_bible_all)

print "Top 10 word pairs:"
print rdd_combined.map(lambda (k, v): (v, k)).sortByKey(False).take(10)
```


### v) 20 most frequent words
Create for us the list of 20 most frequently used words common to both texts. In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible. Order your report in descending order starting by the number of occurrences in Ulysses. Present the same data this time ordered by the number of occurrences in the Bible.


```{python eval=FALSE}
print "Top 20 word pairs (Ulysess):"
rdd_combined = rdd_ulysess_all.join(rdd_bible_all)
print rdd_combined.sortBy(lambda a:a[1], False).take(5)

print "Top 20 word pairs (Bible):"
rdd_combined = rdd_bible_all.join(rdd_ulysess_all)
print rdd_combined.sortBy(lambda a:a[1], False).take(5)
```

### vi) Get a random sample
List for us a random samples containing 5% of words in the final RDD.
```{python eval=FALSE}
rdd_combined.takeSample(1, rdd_combined.count()*5/100, 1)
```

## Problem 2 (20%)
Implement problem 1 using DataFrame API.

### i) DF word number pairs
Use Spark transformation and action functions present in *DF API* to transform those texts into DF-s that contain words and numbers of occurrence of those words in respective text. From King James Bible eliminate all verse numbers of the form: *03:019:024*. Eliminate from both RDDs so called *“stop words”*. List for us 30 most frequent words in each RDD (text).

#### Load data into RDD and cleanup
```{python eval=FALSE}
# Import libraries
import findspark
findspark.init("/opt/spark-2.2.0-bin-hadoop2.7")
from pyspark.sql import SparkSession
from pyspark.sql.functions import split          # Function to split data
from pyspark.sql.functions import explode        # Equivalent to flatMap

# Create Session
spark = SparkSession.builder.master("local") \
                    .appName("p2_df").getOrCreate()

# Read data
df_ulysses = spark.read.text("/home/tim/e63-coursework/hw4/data/4300-2.txt")
df_bible = spark.read.text("/home/tim/e63-coursework/hw4/data/bible.txt")
df_stopwords = spark.read.text("/home/tim/e63-coursework/hw4/data/stopwords.csv")

# Cleanup
df_ulysses_cleaned = df_ulysses.select(split(df_ulysses.value,
                                       " ").alias("words"))
df_ulysses_cleaned = df_ulysses_cleaned.select(explode(
                      df_ulysses_cleaned.words).alias("words"))

# Function
from pyspark.sql.functions import regexp_replace, trim, col, lower
def removePunctuation(column):
  return trim(lower(regexp_replace(column,'([^A-Za-z0-9\s+])', ''))).alias('words')


df = df_ulysses_cleaned.select(removePunctuation(col('words')))

import numpy as np
x = np.array(df.collect())
df.count()

df.show()




x.show()

help(df_ulysses_cleaned.words.regexp_replace)
df_ulysses_cleaned.words.regexp_replace(df_ulysses_cleaned['words'],'03:019:024', '')

from pyspark.sql.functions import regexp_replace, col
df = df_ulysses_cleaned.select(regexp_replace(col("words"), "03:019:024", "").alias("words"))
df = df.select(regexp_replace(col("words"), "([^A-Za-z0-9\s+])", "").alias("words"))







sentenceDF = sqlContext.createDataFrame([('Hi, you',),
                               (' Look! No under_score!',),
                               (' * Remove punctuation then spaces * ',)], 
                               ['sentence'])
  # display first original sentence
sentenceDF.show(truncate=False)
 # then sentence with punctuation removed
(sentenceDF
   .select(removePunctuation(col('sentence')))
   .show(truncate=False))











trim(lower(regexp_replace(column, '[^A-Za-z0-9\s+]', '')))
                .alias('words')


df.words
df.lower()
return trim(lower(regexp_replace(column, '([^\s\w_]|_)+', '')))
                .alias('sentence')
                
                
df = df_ulysses_cleaned.regexp_replace(df_ulysses_cleaned['words'],'03:019:024', '').alias('words') 

df_ulysses_cleaned.count()
df.count()

df = df_ulysses_cleaned.selectExpr("regexp_replace(words,'(03:019:024)', '')").alias("words")


df = df.selectExpr("Name as name", "regexp_replace(words,'(03:019:024) as age")
df.show()

df = df.selectExpr("regexp_replace(words,'([^A-Za-z0-9\s+])', '')")

df.show()



  import re # Import regex library
  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse
  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\s+])', '', rdd_words_clean1) # Nonwords  
  rdd_words_split = rdd_words_clean2.split(' ') # Split data
  return [word.lower() for word in rdd_words_split if word != ''] # Lower case


df_ulysses_cleaned = df_ulysses.select(split(df_ulysses.value,
                                       " ").alias("words"))

# Create one row per word
df_word = df_words.select(explode(df_words.words).alias("words"))

# Remove empy lines
df_word = df_word.filter('words != Null or words != ""')


# Cleanup function
def clean_up(rdd_words):
  import re # Import regex library
  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse
  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\s+])', '', rdd_words_clean1) # Nonwords  
  rdd_words_split = rdd_words_clean2.split(' ') # Split data
  return [word.lower() for word in rdd_words_split if word != ''] # Lower case



# Filter values
df_linematch = df_ulysses.filter(df_ulysses.value.contains('afternoon') |
                                 df_ulysses.value.contains('night') |
                                 df_ulysses.value.contains('morning'))


```

## Problem 3 (30%)
Consider attached files *transactions.txt* and *products.txt*.

### i) Load data
Each line in *transactions.txt* file contains a *transaction date*, *time*, *customer id*, *product id*, *quantity bought* and *price paid*, delimited with hash (#) sign. Each line in file *products.txt* contains *product id*, *product name*, *unit price* and *quantity available* in the store. Bring those data in Spark and organize it as DataFrames with named columns.

### ii) Largest spending
Using either DataFrame methods or plain SQL statements find 5 customers with the largest spent on the day. Find the names of the products each of those 5 customers bought.

### iii) Total number sold
Find the names and total number sold of 10 most popular products. Order products once per the number sold and then by the total value (quanity*price) sold. 

## Problem 4 (20%)
Implement problem 3 using RDD APIs.
