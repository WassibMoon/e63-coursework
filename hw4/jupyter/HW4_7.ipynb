{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (30%)\n",
    "Consider two attached text files: *bible.txt* and *4300.txt*. The first contains ASCII text of King James Bible and the other the text of James Joyceâs novel Ulysses.\n",
    "\n",
    "### i) Download stop words\n",
    "Download and parse a list of *stop words* from the web page: http://www.lextek.com/manuals/onix/stopwords1.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download page\n",
    "page = requests.get(\"http://www.lextek.com/manuals/onix/stopwords1.html\")\n",
    "\n",
    "# Parse page\n",
    "html = BeautifulSoup(page.content, 'html.parser').pre\n",
    "text = html.get_text().split()\n",
    "\n",
    "# Remove introduction\n",
    "stopwords = text[21:len(text)]\n",
    "\n",
    "## Export data to a datafile\n",
    "result_file = open(\"stopwords.csv\", 'w')\n",
    "for i in stopwords:\n",
    "  result_file.write(i + \"\\n\")\n",
    "\n",
    "result_file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) RDD word number pairs\n",
    "Use Spark transformation and action functions present in *RDD API* to transform those texts into RDD-s that contain words and numbers of occurrence of those words in respective text. From King James Bible eliminate all verse numbers of the form: *03:019:024*. Eliminate from both RDDs so called *stop words*. List for us 30 most frequent words in each RDD (text).\n",
    "\n",
    "#### Cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup function\n",
    "def clean_up(rdd_words):\n",
    "  import re # Import regex library\n",
    "  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse\n",
    "  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\\s+])', '', rdd_words_clean1) # Nonwords  \n",
    "  rdd_words_split = rdd_words_clean2.split(' ') # Split data\n",
    "  return [word.lower() for word in rdd_words_split if word != ''] # Lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Startup RDD Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Start session\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"rdd\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into RDD and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "rdd_ulysses = sc.textFile(\"4300-2.txt\")\n",
    "rdd_bible = sc.textFile(\"bible.txt\")\n",
    "rdd_stopwords = sc.textFile(\"stopwords.csv\")\n",
    "\n",
    "# Clean data and remove stopwords and verse number\n",
    "rdd_ulysess = rdd_ulysses.flatMap(clean_up)\n",
    "rdd_ulysess_cleaned = rdd_ulysess.subtract(rdd_stopwords)\n",
    "\n",
    "rdd_bible = rdd_bible.flatMap(clean_up)\n",
    "rdd_bible_cleaned = rdd_bible.subtract(rdd_stopwords)\n",
    "\n",
    "# Number of occurence (Mapreduce)\n",
    "rdd_ulysess_all = rdd_ulysess_cleaned.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)\n",
    "rdd_bible_all = rdd_bible_cleaned.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Top 30 Word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible - Top 30 word pairs:\n",
      "[('unto', 8997), ('lord', 7830), ('thou', 5474), ('thy', 4600), ('god', 4443), ('ye', 3982), ('thee', 3826), ('israel', 2565), ('son', 2370), ('king', 2270), ('hath', 2264), ('people', 2145), ('house', 2024), ('children', 1802), ('day', 1734), ('land', 1718), ('shalt', 1616), ('hand', 1466), ('saying', 1445), ('behold', 1326), ('saith', 1262), ('sons', 1116), ('hast', 1070), ('david', 1015), ('earth', 987), ('jesus', 983), ('father', 979), ('thine', 938), ('name', 930), ('thereof', 906)]\n",
      "Ulysess - Top 30 word pairs:\n",
      "[('bloom', 2798), ('stephen', 1511), ('time', 1146), ('yes', 1082), ('eyes', 987), ('hand', 918), ('street', 879), ('little', 870), ('father', 831), ('day', 753), ('round', 717), ('night', 696), ('head', 666), ('sir', 657), ('dont', 656), ('god', 654), ('name', 651), ('im', 606), ('look', 594), ('life', 583), ('hes', 582), ('john', 582), ('thats', 576), ('poor', 558), ('woman', 558), ('tell', 532), ('voice', 531), ('ill', 522), ('dedalus', 522), ('house', 511)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bible - Top 30 word pairs:\")\n",
    "print(rdd_bible_all.take(30))\n",
    "\n",
    "print(\"Ulysess - Top 30 word pairs:\")\n",
    "print(rdd_ulysess_all.take(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Get unique words\n",
    "Create RDD-s that contain only words unique for each of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43956"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get distinct values\n",
    "rdd_ulysess_dist = rdd_ulysess_cleaned.distinct()\n",
    "rdd_bible_dist = rdd_bible_cleaned.distinct()\n",
    "\n",
    "# Number of unique words\n",
    "rdd_ulysess_dist.count()\n",
    "rdd_bible_dist.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) Get common words\n",
    "Finally create an RDD that contains only the words common to both texts. In latest RDD preserve numbers of occurrences in two texts. In other words a row in your RDD will look like (love 45 32). Print or store the words and the numbers of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Words:\n",
      "[('zion', (15, 152)), ('zealous', (6, 8)), ('zeal', (9, 16)), ('youths', (9, 2)), ('youthful', (18, 1)), ('youth', (93, 70)), ('yourselves', (6, 191)), ('yonder', (6, 7)), ('yokefellow', (3, 1)), ('yoke', (15, 59))]\n"
     ]
    }
   ],
   "source": [
    "rdd_combined = rdd_ulysess_all.join(rdd_bible_all)\n",
    "\n",
    "print(\"Common Words:\")\n",
    "print(rdd_combined.sortByKey(False).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v) 20 most frequent words\n",
    "Create for us the list of 20 most frequently used words common to both texts. In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible. Order your report in descending order starting by the number of occurrences in Ulysses. Present the same data this time ordered by the number of occurrences in the Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 word pairs (Ulysess):\n",
      "[('stephen', (1511, 7)), ('time', (1146, 623)), ('yes', (1082, 4)), ('eyes', (987, 503)), ('hand', (918, 1466))]\n",
      "Top 20 word pairs (Bible):\n",
      "[('unto', (8997, 15)), ('lord', (7830, 447)), ('thou', (5474, 161)), ('thy', (4600, 141)), ('god', (4443, 654))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 word pairs (Ulysess):\")\n",
    "rdd_combined = rdd_ulysess_all.join(rdd_bible_all)\n",
    "print(rdd_combined.sortBy(lambda a:a[1], False).take(5))\n",
    "\n",
    "print(\"Top 20 word pairs (Bible):\")\n",
    "rdd_combined = rdd_bible_all.join(rdd_ulysess_all)\n",
    "print(rdd_combined.sortBy(lambda a:a[1], False).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi) Get a random sample\n",
    "List for us a random samples containing 5% of words in the final RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 percent sample of common words in both books\n",
      "[('warrior', (1, 6)), ('fro', (25, 60)), ('lapwing', (2, 15)), ('prophesying', (6, 3)), ('desperate', (2, 3)), ('kinsmen', (7, 3)), ('solicitation', (2, 3)), ('breach', (26, 21)), ('acceptance', (1, 3)), ('food', (55, 78)), ('rolls', (1, 21)), ('pence', (5, 21)), ('disk', (2, 26)), ('anchors', (3, 3)), ('breathed', (4, 18)), ('suppose', (10, 342)), ('applicable', (6, 3)), ('cursed', (72, 24)), ('liked', (1, 93)), ('youths', (2, 9)), ('laying', (13, 21)), ('courage', (20, 24)), ('couple', (10, 36)), ('security', (1, 12)), ('reared', (10, 9)), ('genealogy', (15, 3)), ('issue', (40, 39)), ('97', (2, 3)), ('discomfited', (9, 3)), ('candles', (1, 24)), ('ought', (97, 186)), ('available', (4, 9)), ('virgins', (23, 51)), ('understand', (93, 110)), ('halting', (1, 3)), ('preferred', (5, 18)), ('dine', (3, 3)), ('unmerciful', (1, 3)), ('dying', (6, 107)), ('treasure', (37, 18)), ('soon', (65, 96)), ('repent', (46, 12)), ('enlargement', (1, 3)), ('revealed', (38, 33)), ('bell', (4, 81)), ('brook', (39, 3)), ('forbearance', (2, 6)), ('cakes', (25, 33)), ('coat', (25, 83)), ('liar', (13, 15)), ('affect', (2, 6)), ('downloading', (2, 1)), ('step', (2, 78)), ('ravenous', (3, 6)), ('shining', (11, 60)), ('uncles', (7, 3)), ('wealth', (27, 27)), ('famine', (96, 8)), ('softer', (1, 18)), ('spouses', (2, 3)), ('wife', (396, 401)), ('shout', (36, 39)), ('comforts', (2, 9)), ('baskets', (15, 6)), ('belong', (12, 12)), ('amen', (78, 39)), ('feeling', (2, 87)), ('jewels', (25, 9)), ('spin', (3, 6)), ('charity', (28, 12)), ('treason', (5, 3)), ('sirs', (7, 9)), ('pains', (4, 21)), ('understood', (37, 45)), ('increased', (49, 12)), ('stump', (4, 12)), ('ice', (3, 9)), ('impotent', (4, 6)), ('fort', (6, 12)), ('twenty', (293, 87)), ('following', (49, 81)), ('happier', (1, 9)), ('precept', (11, 3)), ('requirements', (8, 6)), ('37', (4, 3)), ('chance', (6, 60)), ('waited', (35, 63)), ('immediate', (4, 27)), ('girls', (1, 168)), ('curse', (101, 57)), ('communing', (2, 3)), ('decked', (6, 3)), ('telling', (3, 126)), ('size', (5, 39)), ('lifetime', (3, 12)), ('express', (3, 27)), ('goodman', (6, 6)), ('penny', (9, 117)), ('bedad', (2, 6)), ('childish', (1, 3)), ('grove', (17, 15)), ('45', (4, 11)), ('slightly', (2, 60)), ('stricken', (18, 6)), ('imagination', (14, 24)), ('armour', (24, 12)), ('childhood', (2, 18)), ('graven', (55, 9)), ('trust', (134, 30)), ('diadem', (4, 3)), ('refrain', (9, 9)), ('mused', (1, 18)), ('burden', (69, 9)), ('royal', (29, 141)), ('deck', (2, 15)), ('blasted', (5, 9)), ('caves', (7, 3)), ('olive', (38, 18)), ('carriages', (3, 21)), ('rider', (7, 3)), ('appears', (2, 81)), ('exalted', (64, 6)), ('gap', (1, 30)), ('gracious', (31, 12)), ('apply', (6, 14)), ('putting', (17, 84)), ('adulterer', (3, 9)), ('fatherless', (43, 3)), ('howbeit', (64, 3)), ('bearing', (22, 75)), ('clean', (133, 123)), ('childless', (7, 3)), ('greatness', (32, 3)), ('giver', (2, 9)), ('statute', (35, 15)), ('choler', (2, 3)), ('impediment', (1, 6)), ('beetle', (1, 6)), ('remembered', (57, 74)), ('sporting', (2, 12)), ('wrote', (62, 126)), ('nostrils', (15, 48)), ('speech', (49, 93)), ('elder', (20, 24)), ('resorted', (5, 6)), ('released', (4, 3)), ('eyes', (503, 987)), ('pulled', (7, 54)), ('proportion', (3, 15)), ('hook', (5, 30)), ('thummim', (5, 3)), ('composition', (2, 12)), ('courts', (25, 12)), ('plain', (79, 87)), ('pictures', (3, 33)), ('precious', (76, 18)), ('extreme', (1, 27)), ('herd', (22, 6)), ('husbands', (25, 27)), ('contend', (14, 6)), ('creatures', (12, 15)), ('adorned', (4, 3)), ('stephen', (7, 1511)), ('harlots', (10, 15)), ('saying', (1445, 171)), ('noses', (2, 9)), ('rites', (1, 3)), ('heels', (4, 69)), ('crying', (31, 30)), ('ranges', (4, 3)), ('fan', (8, 45)), ('monuments', (1, 9)), ('scab', (7, 6)), ('vestments', (2, 3)), ('self', (6, 33)), ('therein', (229, 15)), ('errors', (6, 21)), ('spread', (111, 33)), ('living', (147, 99)), ('bashan', (59, 3)), ('anointed', (98, 3)), ('chaste', (3, 12)), ('apparently', (1, 15)), ('ruins', (3, 12)), ('continuing', (4, 3)), ('thunder', (19, 30)), ('vigilant', (2, 12)), ('summer', (27, 105)), ('carried', (145, 63)), ('sail', (8, 35)), ('purposes', (5, 21)), ('flaming', (9, 15)), ('sealing', (1, 3)), ('detest', (1, 6)), ('bleating', (1, 6)), ('fiery', (20, 6)), ('kiss', (20, 138)), ('2001', (2, 17)), ('village', (10, 6)), ('protection', (1, 6)), ('vanities', (13, 9)), ('inferior', (4, 6)), ('lamp', (13, 84)), ('befall', (9, 3)), ('husband', (120, 129)), ('support', (10, 15)), ('carry', (92, 51)), ('standing', (55, 132)), ('situation', (2, 15)), ('believers', (2, 6)), ('authority', (37, 21)), ('chimney', (1, 9)), ('resemblance', (1, 6)), ('quarrel', (4, 9)), ('sacrilege', (1, 3)), ('stay', (33, 45)), ('emmanuel', (1, 3)), ('calfs', (1, 9)), ('scrape', (3, 9)), ('sheba', (32, 6)), ('uttered', (17, 3)), ('gold', (417, 279)), ('joined', (43, 15)), ('hell', (54, 207)), ('mend', (1, 9)), ('unicorn', (6, 3)), ('grass', (62, 39)), ('sober', (12, 42)), ('reading', (8, 111)), ('images', (72, 21)), ('twinkling', (1, 15)), ('benches', (1, 6)), ('burnished', (1, 6)), ('shaved', (4, 15)), ('priest', (497, 120)), ('links', (6, 24)), ('multiplied', (44, 12)), ('spiritual', (28, 18)), ('mouse', (2, 9)), ('sad', (11, 135)), ('depart', (125, 6)), ('extend', (2, 3)), ('warm', (8, 156)), ('satisfaction', (2, 45)), ('sickly', (1, 3)), ('injured', (1, 3)), ('beginning', (108, 60)), ('ignorant', (17, 12)), ('conceiving', (1, 3)), ('serving', (7, 6)), ('wag', (3, 6)), ('governors', (23, 9)), ('profit', (47, 15)), ('profane', (33, 3)), ('sir', (12, 657)), ('timber', (26, 12)), ('brother', (367, 171)), ('vulture', (2, 9)), ('servest', (2, 3)), ('distributed', (14, 7)), ('approached', (2, 18)), ('enquire', (52, 3)), ('obscure', (1, 12)), ('bought', (44, 87)), ('dreams', (21, 24)), ('consent', (15, 9)), ('descending', (8, 12)), ('devised', (12, 6)), ('engines', (2, 9)), ('milk', (48, 153)), ('edge', (56, 42)), ('visions', (24, 6)), ('cottages', (1, 3)), ('till', (169, 402)), ('fed', (31, 24)), ('divisions', (17, 3)), ('thrust', (50, 39)), ('intelligence', (1, 51)), ('dog', (15, 169)), ('offence', (19, 24)), ('examination', (1, 6)), ('cloven', (2, 6)), ('shoulder', (38, 99)), ('distressed', (11, 3)), ('kinds', (10, 33)), ('weight', (58, 66)), ('thirsty', (17, 9)), ('stealeth', (3, 6))]\n"
     ]
    }
   ],
   "source": [
    "rdd_5perc = format(rdd_combined.takeSample(False, int(rdd_combined.count() * 5/100), seed=123))\n",
    "print(\"5 percent sample of common words in both books\")\n",
    "print(rdd_5perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20%)\n",
    "Implement problem 1 using DataFrame API.\n",
    "\n",
    "### i) DF word number pairs\n",
    "Use Spark transformation and action functions present in *DF API* to transform those texts into DF-s that contain words and numbers of occurrence of those words in respective text. From King James Bible eliminate all verse numbers of the form: *03:019:024*. Eliminate from both RDDs so called *stop words*. List for us 30 most frequent words in each DF (text).\n",
    "\n",
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "\n",
    "def removePunctuation(column):\n",
    "  return trim(lower(regexp_replace(column,'([^A-Za-z0-9\\s+])', ''))).alias('words')\n",
    "\n",
    "# Cleanup function\n",
    "def clean_up(rdd_words):\n",
    "  import re # Import regex library\n",
    "  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse\n",
    "  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\\s+])', '', rdd_words_clean1) # Nonwords  \n",
    "  rdd_words_split = rdd_words_clean2.split(' ') # Split data\n",
    "  return [word.lower() for word in rdd_words_split if word != ''] # Lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split          # Function to split data\n",
    "from pyspark.sql.functions import explode        # Equivalent to flatMap\n",
    "\n",
    "# Create Session\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "                    .appName(\"df\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_ulysses = spark.read.text(\"4300-2.txt\")\n",
    "df_bible = spark.read.text(\"bible.txt\")\n",
    "df_stopwords = spark.read.text(\"stopwords.csv\")\n",
    "\n",
    "# Select words\n",
    "df_ulysses_all = df_ulysses.select(split(df_ulysses.value, \" \").alias(\"words\"))\n",
    "df_ulysses_all = df_ulysses_all.select(explode(df_ulysses_all.words).alias(\"words\"))\n",
    "df_ulysses_all = df_ulysses_all.select(removePunctuation(col('words')))\n",
    "df_ulysses_all = df_ulysses_all.filter('words != Null or words != \"\"')\n",
    "\n",
    "df_bible_all = df_bible.select(split(df_bible.value, \" \").alias(\"words\"))\n",
    "df_bible_all = df_bible_all.select(explode(df_bible_all.words).alias(\"words\"))\n",
    "df_bible_all = df_bible_all.select(removePunctuation(col('words')))\n",
    "df_bible_all = df_bible_all.filter('words != Null or words != \"\"')\n",
    "\n",
    "# Remove stopwords\n",
    "df_ulysses_cleaned = df_ulysses_all.join(df_stopwords, df_ulysses_all.words == df_stopwords.value, 'left_anti').select(df_ulysses_all.words)\n",
    "df_bible_cleaned = df_bible_all.join(df_stopwords, df_bible_all.words == df_stopwords.value, 'left_anti').select(df_bible_all.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most frequent words: \n",
      "+-------+-----+\n",
      "|  words|count|\n",
      "+-------+-----+\n",
      "|  bloom| 2798|\n",
      "|stephen| 1511|\n",
      "|   time| 1146|\n",
      "|    yes| 1082|\n",
      "|   eyes|  987|\n",
      "|   hand|  918|\n",
      "| street|  879|\n",
      "| little|  870|\n",
      "| father|  831|\n",
      "|    day|  753|\n",
      "|  round|  717|\n",
      "|  night|  696|\n",
      "|   head|  666|\n",
      "|    sir|  657|\n",
      "|   dont|  656|\n",
      "|    god|  654|\n",
      "|   name|  651|\n",
      "|     im|  606|\n",
      "|   look|  594|\n",
      "|   life|  583|\n",
      "|    hes|  582|\n",
      "|   john|  582|\n",
      "|  thats|  576|\n",
      "|   poor|  558|\n",
      "|  woman|  558|\n",
      "|   tell|  532|\n",
      "|  voice|  531|\n",
      "|    ill|  522|\n",
      "|dedalus|  522|\n",
      "|  house|  511|\n",
      "+-------+-----+\n",
      "only showing top 30 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get frequent word pair\n",
    "df_ulysses_unique = df_ulysses_cleaned.groupBy(\"words\").count()\n",
    "df_ulysses_unique = df_ulysses_unique.orderBy([\"count\"], ascending=False)\n",
    "print(\"30 Most frequent words: \")\n",
    "print(df_ulysses_unique.show(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most frequent words: \n",
      "+--------+-----+\n",
      "|   words|count|\n",
      "+--------+-----+\n",
      "|    unto| 8997|\n",
      "|    lord| 7830|\n",
      "|    thou| 5474|\n",
      "|     thy| 4600|\n",
      "|     god| 4443|\n",
      "|      ye| 3982|\n",
      "|    thee| 3826|\n",
      "|  israel| 2565|\n",
      "|     son| 2370|\n",
      "|    king| 2270|\n",
      "|    hath| 2264|\n",
      "|  people| 2145|\n",
      "|   house| 2024|\n",
      "|children| 1802|\n",
      "|     day| 1734|\n",
      "|    land| 1718|\n",
      "|   shalt| 1616|\n",
      "|    hand| 1466|\n",
      "|  saying| 1445|\n",
      "|  behold| 1326|\n",
      "|   saith| 1262|\n",
      "|    sons| 1116|\n",
      "|    hast| 1070|\n",
      "|   david| 1015|\n",
      "|   earth|  987|\n",
      "|   jesus|  983|\n",
      "|  father|  979|\n",
      "|   thine|  938|\n",
      "|    name|  930|\n",
      "| thereof|  906|\n",
      "+--------+-----+\n",
      "only showing top 30 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get frequent word pair\n",
    "df_bible_unique = df_bible_cleaned.groupBy(\"words\").count()\n",
    "df_bible_unique = df_bible_unique.orderBy([\"count\"], ascending=False)\n",
    "print(\"30 Most frequent words: \")\n",
    "print(df_bible_unique.show(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ii) Get unique words\n",
    "Create DF-s that contain only words unique for each of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30038"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ulysess_dist = df_ulysses_all.distinct()\n",
    "df_ulysess_dist.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44285"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bible_dist = df_bible_all.distinct()\n",
    "df_bible_dist.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Get common words\n",
    "Finally create an DF that contains only the words common to both texts. In latest DF preserve numbers of occurrences in two texts. In other words a row in your DF will look like (love 45 32). Print or store the words and the numbers of occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-----+\n",
      "|  words|count|  words|count|\n",
      "+-------+-----+-------+-----+\n",
      "|stephen| 1511|stephen|    7|\n",
      "|   time| 1146|   time|  623|\n",
      "|    yes| 1082|    yes|    4|\n",
      "|   eyes|  987|   eyes|  503|\n",
      "|   hand|  918|   hand| 1466|\n",
      "+-------+-----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined = df_ulysses_unique.join(df_bible_unique, df_ulysses_unique.words == df_bible_unique.words, 'inner')\n",
    "df_combined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) 20 most frequent words\n",
    "Create for us the list of 20 most frequently used words common to both texts. In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible. Order your report in descending order starting by the number of occurrences in Ulysses. Present the same data this time ordered by the number of occurrences in the Bible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.select(['word', 'bible_count']).orderBy(col('bible_count').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5771\n"
     ]
    }
   ],
   "source": [
    "print(df_combined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible.\"\n",
    "print(bible_combined_df.show(20))\n",
    "print(bible_combined_df.agg(sum('bible_count').alias('sum_bible_count')).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v) Get a random sample\n",
    "List for us a random samples containing 5% of words in the final DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order your report in descending order starting by the number of occurrences in Ulysses.\n",
    "print \"Order your report in descending order starting by the number of occurrences in Ulysses.\"\n",
    "ulysses_combined_df = combined_df.select(['ulysses_word', 'ulysses_count']).orderBy(col('ulysses_count').desc())\n",
    "print(ulysses_combined_df.show(20))\n",
    "print(ulysses_combined_df.agg(sum('ulysses_count').alias('sum_ulysses_count')).show())\n",
    "\n",
    "# List for us a random samples containing 5% of words in the final RDD.\n",
    "print \"List for us a random samples containing 5% of words in the final RDD.\"\n",
    "final_df_sample = bible_combined_df.sample(False, 0.5, 13)\n",
    "print(final_df_sample.show())\n",
    "print(final_df_sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3 (30%)\n",
    "Consider attached files *transactions.txt* and *products.txt*.\n",
    "\n",
    "### i) Load data\n",
    "Each line in *transactions.txt* file contains a *transaction date*, *time*, *customer id*, *product id*, *quantity bought* and *price paid*, delimited with hash (#) sign. Each line in file *products.txt* contains *product id*, *product name*, *unit price* and *quantity available* in the store. Bring those data in Spark and organize it as DataFrames with named columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_transactions = spark.read.csv(\"transactions.txt\", sep=\"#\")\n",
    "df_products = spark.read.csv(\"products.txt\", sep=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_transactions = df_transactions.withColumnRenamed('_c0', \"transaction_date\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c1', \"time\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c2', \"customer_id\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c3', \"product_id\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c4', \"quantity_bought\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c5', \"price_paid\")\n",
    "\n",
    "df_products = df_products.withColumnRenamed('_c0', \"product_id\")\n",
    "df_products = df_products.withColumnRenamed('_c1', \"product_name\")\n",
    "df_products = df_products.withColumnRenamed('_c2', \"unit_price\")\n",
    "df_products = df_products.withColumnRenamed('_c3', \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Largest spending\n",
    "Using either DataFrame methods or plain SQL statements find 5 customers with the largest spent on the day. Find the names of the products each of those 5 customers bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_transactions.groupBy(\"customer id\").sum().show()\n",
    "df_cust_spend=df_transactions.groupBy('customer_id', 'transaction_date').agg({'price_paid': 'sum'})\n",
    "df_cust_spend = df_cust_spend.orderBy('sum(price_paid)', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tables\n",
    "df_cust_spend.createOrReplaceTempView(\"tbl_cust_spend\")\n",
    "df_transactions.createOrReplaceTempView(\"tbl_transactions\")\n",
    "df_products.createOrReplaceTempView(\"tbl_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------------+\n",
      "|customer_id|transaction_date|   sum(price_paid)|\n",
      "+-----------+----------------+------------------+\n",
      "|         76|      2015-03-30|100049.00000000001|\n",
      "|         53|      2015-03-30| 88829.76000000001|\n",
      "|         56|      2015-03-30|          85906.94|\n",
      "|         51|      2015-03-30|          83312.12|\n",
      "|         31|      2015-03-30|          83202.61|\n",
      "+-----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top5 = spark.sql(\"SELECT * FROM tbl_cust_spend LIMIT 5\")\n",
    "df_top5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-----------+----------+---------------+----------+\n",
      "|transaction_date|    time|customer_id|product_id|quantity_bought|price_paid|\n",
      "+----------------+--------+-----------+----------+---------------+----------+\n",
      "|      2015-03-30| 6:55 AM|         51|        68|              1|   9506.21|\n",
      "|      2015-03-30| 7:39 PM|         99|        86|              5|   4107.59|\n",
      "|      2015-03-30|11:57 AM|         79|        58|              7|   2987.22|\n",
      "|      2015-03-30|12:46 AM|         51|        50|              6|   7501.89|\n",
      "|      2015-03-30|11:39 AM|         86|        24|              5|    8370.2|\n",
      "+----------------+--------+-----------+----------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|        product_name|\n",
      "+-----------+--------------------+\n",
      "|          1|SAMSUNG LED TV 42...|\n",
      "|          1|ROBITUSSIN PEAK C...|\n",
      "|          1|    LEGO Minifigures|\n",
      "|          1|           Glipizide|\n",
      "|          1|Scrub Care Povido...|\n",
      "|          1|Medal Of Honor Al...|\n",
      "|          1|Notebook Lenovo U...|\n",
      "|          1|        LEGO Technic|\n",
      "|          1|PC HP 490PD MT, D...|\n",
      "|         10|              Ativan|\n",
      "|         10|   LEGO Galaxy Squad|\n",
      "|         10|SAMSUNG LED TV 32...|\n",
      "|         10|          Dictionary|\n",
      "|         10|ROBITUSSIN PEAK C...|\n",
      "|         10|Procesor Intel Co...|\n",
      "|         10|GAM X360 Hitman A...|\n",
      "|        100|    chest congestion|\n",
      "|        100|PC HP 490PD MT, D...|\n",
      "|        100|     LEGO The Hobbit|\n",
      "|        100|Roller Derby Roll...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top5_products = df_transactions.join(df_top5, df_transactions.customer_id == df_top5.customer_id, \"left\").select(df_transactions.customer_id, df_transactions.product_id)\n",
    "df_top5_list = df_top5_products.join(df_products, df_top5_products.product_id == df_products.product_id, \"left\").select(df_top5_products.customer_id, df_products.product_name)\n",
    "df_top5_list.orderBy(\"customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Total number sold\n",
    "Find the names and total number sold of 10 most popular products. Order products once per the number sold and then by the total value (quanity*price) sold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+------------------+\n",
      "|        product_name|quantity|unit_price|       Total value|\n",
      "+--------------------+--------+----------+------------------+\n",
      "|Notebook Lenovo U...|   226.0|    461.08|         104204.08|\n",
      "|SAMSUNG LED TV 39...|   142.0|   2531.15|          359423.3|\n",
      "|               Jafra|   102.0|   3715.07|         378937.14|\n",
      "|            Jantoven|   102.0|    3255.4|          332050.8|\n",
      "|Far Cry 4 Limited...|   101.0|    711.88|          71899.88|\n",
      "|Roller Derby Roll...|    91.0|   7783.79|         708324.89|\n",
      "|Procesor Intel Co...|    90.0|   4570.99|          411389.1|\n",
      "|  Sony Playstation 3|    88.0|   5088.35|447774.80000000005|\n",
      "|    chest congestion|    84.0|   1305.04|         109623.36|\n",
      "|Barbie Beach Ken ...|    82.0|    742.84|60912.880000000005|\n",
      "+--------------------+--------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List the sum of sold products\n",
    "df_sum_products=df_transactions.groupBy('product_id').agg({'quantity_bought': 'sum'})\n",
    "df_sum_products = df_sum_products.orderBy('sum(quantity_bought)', ascending=False)\n",
    "\n",
    "# Get top ten results\n",
    "df_sum_products.createOrReplaceTempView(\"tbl_sum_products\")\n",
    "df_top10_products = spark.sql(\"SELECT * FROM tbl_sum_products LIMIT 10\")\n",
    "\n",
    "# Calculate the total value\n",
    "df_products_distinct = df_products.select(df_products.product_id, df_products.product_name, df_products.unit_price).distinct()\n",
    "df_top10_products = df_top10_products.join(df_products_distinct, df_top10_products.product_id == df_products_distinct.product_id, \"left\")\n",
    "df_top10_products = df_top10_products.select(df_top10_products['product_name'], df_top10_products['sum(quantity_bought)'].alias(\"quantity\"), df_top10_products['unit_price'], (df_top10_products['sum(quantity_bought)'] * df_top10_products['unit_price']).alias(\"Total value\"))\n",
    "df_top10_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20%)\n",
    "Implement problem 3 using RDD APIs.\n",
    "\n",
    "### i) Load data\n",
    "Each line in *transactions.txt* file contains a *transaction date*, *time*, *customer id*, *product id*, *quantity bought* and *price paid*, delimited with hash (#) sign. Each line in file *products.txt* contains *product id*, *product name*, *unit price* and *quantity available* in the store. Bring those data in Spark and organize it as DataFrames with named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "rdd_transactions = sc.textFile(\"transactions.txt\")\n",
    "rdd_transactions = rdd_transactions.map(lambda x: x.split(\"#\"))\n",
    "rdd_transactions = rdd_transactions.map(lambda x: Row(transaction_date = x[0], time = x[1], customer_id = int(x[2]), product_id = int(x[3]), quantity_bought = int(x[4]), price_paid = float(x[5])))\n",
    "\n",
    "rdd_products = sc.textFile(\"products.txt\")\n",
    "rdd_products = rdd_products.map(lambda x: x.split(\"#\"))\n",
    "rdd_products = rdd_products.map(lambda x: Row(product_id = int(x[0]), product_name = x[1], unit_price = float(x[2]), quantity = int(x[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Largest spending\n",
    "Using either DataFrame methods or plain SQL statements find 5 customers with the largest spent on the day. Find the names of the products each of those 5 customers bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"transaction_date time customer_id product_id quantity_bought price_paid\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Create schema\n",
    "sch_transactions = spark.createDataFrame(rdd_transactions, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "sch_transactions.createOrReplaceTempView(\"tbl_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_id=51, price_paid=9506.21, product_id=68, quantity_bought=1, time='6:55 AM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=99, price_paid=4107.59, product_id=86, quantity_bought=5, time='7:39 PM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=79, price_paid=2987.22, product_id=58, quantity_bought=7, time='11:57 AM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=51, price_paid=7501.89, product_id=50, quantity_bought=6, time='12:46 AM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=86, price_paid=8370.2, product_id=24, quantity_bought=5, time='11:39 AM', transaction_date='2015-03-30')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_transactions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-666516511370>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-666516511370>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    rdd_sum = rdd_transactions.reduce(lambda x[0], x[1]: x[0] + x[1])\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transactions_rdd = sc.textFile(\"file:////Users/swaite/Stirling/CSIE-63/assignment-4/data/inputs/transactions.txt\") \\\n",
    "                     .map(lambda x: x.split(\"#\"))\n",
    "transactions_rdd = transactions_rdd.map(lambda x:\n",
    "                                        Row(\n",
    "                                            transaction_date=str(x[0]),\n",
    "                                            time=str(x[1]),\n",
    "                                            customer_id=int(x[2]),\n",
    "                                            product_id=int(x[3]),\n",
    "                                            quantity_bought=int(x[4]),\n",
    "                                            price_paid=float(x[5])\n",
    "                                        ))\n",
    "transactions_df = spark.createDataFrame(transactions_rdd)\n",
    "print(transactions_df.show(10))\n",
    "\n",
    "\n",
    "# Each line in file products.txt contains:\n",
    "#       product id,\n",
    "#       product name,\n",
    "#       unit price,\n",
    "#       quantity\n",
    "# available in the store.\n",
    "# Bring those data in Spark and organize it as DataFrames with named columns.\n",
    "\n",
    "products_rdd = sc.textFile(\"file:////Users/swaite/Stirling/CSIE-63/assignment-4/data/inputs/products.txt\")\\\n",
    "                 .map(lambda x: x.split(\"#\"))\n",
    "products_rdd = products_rdd.map(lambda x:\n",
    "                                Row(\n",
    "                                    product_id=str(x[0]),\n",
    "                                    product_name=str(x[1]),\n",
    "                                    unit_price=float(x[2]),\n",
    "                                    quantity=float(x[3])\n",
    "                                ))\n",
    "products_df = spark.createDataFrame(products_rdd)\n",
    "print(products_df.show(10))\n",
    "\n",
    "# Using either DataFrame methods or plain SQL statements find 5 customers with the largest spent on the day.\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "blah = spark.sql(\n",
    "                    \"\"\"\n",
    "                        SELECT\n",
    "                        customer_id,\n",
    "                        SUM(to_float(quantity_bought) * to_float(price_paid)) AS net_rev\n",
    "                        FROM transactions\n",
    "                        GROUP BY customer_id\n",
    "                        ORDER BY net_rev\n",
    "                    \"\"\")\n",
    "\n",
    "# GROUP BY customer_id\n",
    "# ORDER BY 2 DESC\n",
    "\n",
    "print(blah.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Total number sold\n",
    "Find the names and total number sold of 10 most popular products. Order products once per the number sold and then by the total value (quanity*price) sold. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
