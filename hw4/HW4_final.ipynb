{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (30%)\n",
    "Consider two attached text files: *bible.txt* and *4300.txt*. The first contains ASCII text of King James Bible and the other the text of James Joyce's novel Ulysses.\n",
    "\n",
    "**NOTE:** For this assignment a Docker container with the Jupyter Notebook as an IDE was used. The advantage of this approach is it's reproducabiltiy.\n",
    "\n",
    "### i) Download stop words\n",
    "Download and parse a list of *stop words* from the web page: http://www.lextek.com/manuals/onix/stopwords1.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download page\n",
    "page = requests.get(\"http://www.lextek.com/manuals/onix/stopwords1.html\")\n",
    "\n",
    "# Parse page\n",
    "html = BeautifulSoup(page.content, 'html.parser').pre\n",
    "text = html.get_text().split()\n",
    "\n",
    "# Remove introduction\n",
    "stopwords = text[21:len(text)]\n",
    "\n",
    "## Export data to a datafile\n",
    "result_file = open(\"stopwords.csv\", 'w')\n",
    "for i in stopwords:\n",
    "  result_file.write(i + \"\\n\")\n",
    "\n",
    "result_file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) RDD word number pairs\n",
    "Use Spark transformation and action functions present in *RDD API* to transform those texts into RDD-s that contain words and numbers of occurrence of those words in respective text. From King James Bible eliminate all verse numbers of the form: *03:019:024*. Eliminate from both RDDs so called *stop words*. List for us 30 most frequent words in each RDD (text).\n",
    "\n",
    "#### Cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleanup function\n",
    "def clean_up(rdd_words):\n",
    "  import re # Import regex library\n",
    "  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse\n",
    "  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\\s+])', '',\n",
    "                            rdd_words_clean1) # Nonwords  \n",
    "  rdd_words_split = rdd_words_clean2.split(' ') # Split data\n",
    "  return [word.lower() for word in rdd_words_split if word != ''] # Lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Startup RDD Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Start session\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"rdd\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into RDD and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "rdd_ulysses = sc.textFile(\"4300-2.txt\")\n",
    "rdd_bible = sc.textFile(\"bible.txt\")\n",
    "rdd_stopwords = sc.textFile(\"stopwords.csv\")\n",
    "\n",
    "# Clean data and remove stopwords and verse number\n",
    "rdd_ulysess = rdd_ulysses.flatMap(clean_up)\n",
    "rdd_ulysess_cleaned = rdd_ulysess.subtract(rdd_stopwords)\n",
    "\n",
    "rdd_bible = rdd_bible.flatMap(clean_up)\n",
    "rdd_bible_cleaned = rdd_bible.subtract(rdd_stopwords)\n",
    "\n",
    "# Number of occurence (Mapreduce)\n",
    "rdd_ulysess_all = rdd_ulysess_cleaned.map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)\n",
    "rdd_bible_all = rdd_bible_cleaned.map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Top 30 Word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible - Top 30 word pairs:\n",
      "[('the', 64294), ('and', 51836), ('of', 34868), ('to', 13722), ('that', 12943), ('in', 12785), ('he', 10424), ('shall', 9842), ('for', 9023), ('unto', 8997), ('i', 8854), ('his', 8473), ('a', 8291), ('lord', 7830), ('they', 7382), ('be', 7051), ('is', 7041), ('him', 6659), ('not', 6638), ('them', 6430), ('it', 6159), ('with', 6110), ('all', 5656), ('thou', 5474), ('thy', 4600), ('was', 4524), ('god', 4443), ('which', 4427), ('my', 4368), ('me', 4096)]\n",
      "Ulysess - Top 30 word pairs:\n",
      "[('the', 45043), ('of', 24576), ('and', 21758), ('a', 19569), ('to', 15095), ('in', 14831), ('he', 12080), ('his', 9983), ('i', 8093), ('that', 7846), ('with', 7565), ('it', 7131), ('was', 6400), ('on', 6361), ('for', 5866), ('you', 5842), ('her', 5353), ('him', 4570), ('is', 4369), ('all', 3988), ('by', 3901), ('at', 3890), ('said', 3617), ('as', 3605), ('she', 3402), ('from', 3290), ('they', 3074), ('or', 3054), ('me', 2824), ('bloom', 2798)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bible - Top 30 word pairs:\")\n",
    "print(rdd_bible_all.take(30))\n",
    "\n",
    "print(\"Ulysess - Top 30 word pairs:\")\n",
    "print(rdd_ulysess_all.take(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Get unique words\n",
    "Create RDD-s that contain only words unique for each of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44284"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get distinct values\n",
    "rdd_ulysess_dist = rdd_ulysess_cleaned.distinct()\n",
    "rdd_bible_dist = rdd_bible_cleaned.distinct()\n",
    "\n",
    "# Number of unique words\n",
    "rdd_ulysess_dist.count()\n",
    "rdd_bible_dist.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) Get common words\n",
    "Finally create an RDD that contains only the words common to both texts. In latest RDD preserve numbers of occurrences in two texts. In other words a row in your RDD will look like (love 45 32). Print or store the words and the numbers of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Words:\n",
      "[('zion', (15, 152)), ('zealous', (6, 8)), ('zeal', (9, 16)), ('youths', (9, 2)), ('youthful', (18, 1)), ('youth', (93, 70)), ('yourselves', (6, 191)), ('yours', (57, 12)), ('your', (1503, 1796)), ('younger', (33, 31))]\n"
     ]
    }
   ],
   "source": [
    "rdd_combined = rdd_ulysess_all.join(rdd_bible_all)\n",
    "\n",
    "print(\"Common Words:\")\n",
    "print(rdd_combined.sortByKey(False).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v) 20 most frequent words\n",
    "Create for us the list of 20 most frequently used words common to both texts. In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible. Order your report in descending order starting by the number of occurrences in Ulysses. Present the same data this time ordered by the number of occurrences in the Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 word pairs (Ulysess):\n",
      "[('the', (45043, 64294)), ('of', (24576, 34868)), ('and', (21758, 51836)), ('a', (19569, 8291)), ('to', (15095, 13722))]\n",
      "Top 20 word pairs (Bible):\n",
      "[('the', (64294, 45043)), ('and', (51836, 21758)), ('of', (34868, 24576)), ('to', (13722, 15095)), ('that', (12943, 7846))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 word pairs (Ulysess):\")\n",
    "rdd_combined = rdd_ulysess_all.join(rdd_bible_all)\n",
    "print(rdd_combined.sortBy(lambda a:a[1], False).take(5))\n",
    "\n",
    "print(\"Top 20 word pairs (Bible):\")\n",
    "rdd_combined = rdd_bible_all.join(rdd_ulysess_all)\n",
    "print(rdd_combined.sortBy(lambda a:a[1], False).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi) Get a random sample\n",
    "List for us a random samples containing 5% of words in the final RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 percent sample of common words in both books\n",
      "[('rinsed', (3, 3)), ('oak', (15, 15)), ('unjust', (17, 3)), ('withdrawn', (6, 6)), ('likewise', (107, 9)), ('stuck', (3, 108)), ('tears', (36, 78)), ('rushed', (3, 12)), ('esteemed', (11, 6)), ('bricks', (4, 15)), ('confused', (2, 12)), ('abroad', (80, 18)), ('gardener', (1, 12)), ('whole', (250, 180)), ('moving', (5, 63)), ('brow', (2, 72)), ('brought', (863, 244)), ('kneel', (2, 30)), ('thanks', (73, 132)), ('help', (136, 111)), ('juda', (10, 6)), ('uses', (1, 9)), ('easily', (4, 78)), ('liar', (13, 15)), ('engaged', (1, 54)), ('meet', (134, 132)), ('pulpit', (1, 3)), ('workman', (10, 3)), ('stoning', (1, 3)), ('professed', (2, 3)), ('upbraid', (2, 3)), ('ended', (21, 21)), ('magistrates', (8, 3)), ('wires', (1, 12)), ('wedlock', (1, 6)), ('stink', (8, 15)), ('reckoned', (22, 9)), ('opening', (7, 57)), ('teacher', (6, 6)), ('trench', (8, 3)), ('master', (157, 231)), ('contradicting', (1, 3)), ('roes', (5, 6)), ('immortal', (1, 36)), ('beginning', (108, 60)), ('mourners', (4, 24)), ('vinegar', (13, 12)), ('crop', (2, 15)), ('labours', (13, 9)), ('here', (162, 844)), ('spirits', (46, 30)), ('girded', (33, 3)), ('centurion', (20, 3)), ('blossomed', (1, 3)), ('companions', (22, 21)), ('housetops', (7, 9)), ('contribution', (1, 6)), ('revelation', (13, 6)), ('sewed', (2, 3)), ('multiply', (46, 9)), ('overflow', (13, 3)), ('preached', (61, 3)), ('released', (4, 3)), ('depart', (125, 6)), ('instant', (8, 111)), ('authorities', (1, 6)), ('contentment', (1, 3)), ('booth', (2, 6)), ('epistle', (14, 9)), ('rising', (39, 90)), ('boiled', (3, 24)), ('groaned', (1, 3)), ('unoccupied', (1, 6)), ('saffron', (1, 9)), ('36', (4, 3)), ('appearance', (38, 39)), ('mountains', (178, 21)), ('corn', (102, 8)), ('wayside', (2, 3)), ('sailed', (15, 21)), ('patient', (9, 24)), ('anointed', (98, 3)), ('scroll', (2, 6)), ('beeves', (7, 3)), ('ho', (4, 84)), ('body', (240, 216)), ('damned', (3, 63)), ('sell', (35, 42)), ('conceit', (5, 3)), ('inherited', (6, 3)), ('did', (1006, 1208)), ('described', (4, 18)), ('pounds', (5, 153)), ('or', (1277, 3054)), ('harps', (20, 9)), ('beggarly', (1, 3)), ('unbelief', (16, 6)), ('646221541', (2, 2)), ('solemnly', (2, 33)), ('wants', (2, 138)), ('knees', (30, 69)), ('miserable', (3, 3)), ('pitcher', (12, 3)), ('encourage', (4, 11)), ('backside', (3, 9)), ('killing', (5, 15)), ('spread', (111, 33)), ('henceforth', (33, 9)), ('spouses', (2, 3)), ('miracles', (27, 9)), ('brood', (1, 27)), ('brothers', (36, 54)), ('stripling', (1, 36)), ('dropped', (7, 27)), ('set', (713, 189)), ('methods', (2, 12)), ('pine', (8, 6)), ('height', (62, 33)), ('cock', (12, 45)), ('sayest', (40, 2)), ('esther', (56, 3)), ('express', (3, 27)), ('finding', (10, 9)), ('lighten', (7, 3)), ('pools', (5, 3)), ('babes', (9, 12)), ('glorified', (50, 3)), ('jehovah', (4, 6)), ('windy', (1, 18)), ('curse', (101, 57)), ('world', (287, 456)), ('pisgah', (5, 9)), ('consider', (67, 6)), ('landing', (1, 21)), ('countenance', (53, 30)), ('hardened', (33, 3)), ('admonition', (3, 9)), ('persuasion', (1, 6)), ('submitted', (3, 9)), ('whale', (2, 9)), ('chasing', (1, 3)), ('true', (81, 185)), ('secure', (9, 9)), ('3', (9, 69)), ('fulfilling', (3, 6)), ('perceiving', (3, 18)), ('neighbours', (50, 6)), ('sinners', (48, 6)), ('antiquity', (1, 13)), ('winter', (14, 18)), ('reel', (2, 9)), ('sur', (1, 9)), ('liking', (2, 3)), ('dreams', (21, 24)), ('temple', (204, 30)), ('trumpet', (61, 3)), ('transferred', (1, 9)), ('scales', (10, 6)), ('anger', (234, 12)), ('bank', (14, 75)), ('company', (86, 137)), ('herself', (42, 129)), ('acres', (1, 21)), ('titles', (2, 18)), ('wall', (179, 165)), ('polled', (3, 3)), ('womb', (71, 39)), ('denied', (19, 3)), ('aside', (72, 96)), ('flight', (8, 15)), ('chiding', (1, 6)), ('palace', (48, 30)), ('nearer', (2, 54)), ('lacking', (8, 6)), ('sand', (28, 96)), ('against', (1669, 386)), ('fables', (5, 3)), ('choked', (6, 21)), ('eminent', (4, 18)), ('readiness', (3, 3)), ('pilate', (56, 9)), ('liability', (6, 15)), ('uphold', (8, 3)), ('either', (43, 114)), ('prayer', (109, 42)), ('effect', (14, 63)), ('italy', (4, 6)), ('row', (17, 105)), ('omega', (4, 6)), ('preserver', (1, 3)), ('fruits', (42, 21)), ('chronicles', (44, 3)), ('noise', (88, 63)), ('excluded', (1, 6)), ('wonderful', (21, 66)), ('scoffers', (1, 3)), ('highway', (16, 6)), ('scourge', (12, 6)), ('am', (874, 486)), ('flag', (1, 39)), ('ministering', (9, 3)), ('agreement', (42, 3)), ('sweetness', (5, 3)), ('furthermore', (14, 6)), ('gap', (1, 30)), ('leaped', (8, 9)), ('bodies', (33, 45)), ('cures', (1, 9)), ('got', (7, 552)), ('rule', (66, 33)), ('wanting', (8, 15)), ('staff', (45, 18)), ('comprehended', (3, 3)), ('sound', (89, 78)), ('then', (2169, 1742)), ('fallow', (3, 3)), ('dishonest', (2, 3)), ('desires', (3, 15)), ('additions', (4, 6)), ('statute', (35, 15)), ('sober', (12, 42)), ('ships', (39, 39)), ('dark', (43, 381)), ('worm', (14, 3)), ('trial', (6, 12)), ('citizens', (1, 18)), ('tumbled', (1, 6)), ('loftily', (1, 3)), ('spider', (1, 6)), ('waited', (35, 63)), ('pool', (22, 12)), ('fried', (2, 27)), ('poverty', (15, 15)), ('gush', (1, 6)), ('continual', (33, 6)), ('vanished', (2, 18)), ('judge', (191, 36)), ('conscience', (31, 18)), ('watchtower', (2, 3)), ('walk', (212, 210)), ('envied', (6, 3)), ('more', (688, 954)), ('court', (122, 138)), ('binary', (2, 3)), ('enlarged', (11, 3)), ('aged', (9, 30)), ('feeling', (2, 87)), ('observers', (1, 3)), ('resist', (10, 12)), ('maintained', (1, 6)), ('mend', (1, 9)), ('except', (80, 63)), ('belch', (1, 3)), ('physician', (6, 12)), ('renamed', (2, 3)), ('molten', (39, 6)), ('whoever', (1, 36)), ('prayers', (24, 24)), ('girding', (2, 3)), ('crept', (1, 9)), ('liked', (1, 93)), ('54', (4, 9)), ('hired', (34, 9)), ('dagger', (3, 9)), ('trusty', (1, 6)), ('as', (3540, 3605)), ('forbidden', (3, 6)), ('earthquakes', (3, 3)), ('pans', (4, 6)), ('wave', (32, 18)), ('withheld', (6, 9)), ('fishes', (27, 27)), ('ascend', (13, 6)), ('blue', (50, 246)), ('vilest', (1, 3)), ('reserve', (3, 12)), ('believers', (2, 6)), ('nostrils', (15, 48)), ('terrestrial', (2, 18)), ('eight', (80, 51)), ('extinct', (2, 6)), ('names', (127, 78)), ('visions', (24, 6)), ('associated', (16, 12)), ('pardon', (16, 24)), ('rested', (21, 27)), ('sucklings', (4, 6)), ('abode', (69, 18)), ('shew', (228, 3)), ('leader', (3, 21)), ('hairs', (15, 18)), ('clement', (1, 3)), ('instructing', (1, 3)), ('none', (358, 126)), ('hurt', (63, 48)), ('bottom', (20, 82)), ('sackcloth', (46, 3)), ('conceits', (2, 3)), ('devise', (16, 3)), ('guide', (23, 24)), ('honours', (1, 9)), ('worshipper', (2, 3)), ('enemy', (107, 21)), ('herein', (9, 3))]\n"
     ]
    }
   ],
   "source": [
    "rdd_5perc = format(rdd_combined.takeSample(False,\\\n",
    "                                           int(rdd_combined.count() *\n",
    "                                               5/100), seed=123))\n",
    "print(\"5 percent sample of common words in both books\")\n",
    "print(rdd_5perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20%)\n",
    "Implement problem 1 using DataFrame API.\n",
    "\n",
    "### i) DF word number pairs\n",
    "Use Spark transformation and action functions present in *DF API* to transform those texts into DF-s that contain words and numbers of occurrence of those words in respective text. From King James Bible eliminate all verse numbers of the form: *03:019:024*. Eliminate from both RDDs so called *stop words*. List for us 30 most frequent words in each DF (text).\n",
    "\n",
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "\n",
    "def removePunctuation(column):\n",
    "  return trim(lower(regexp_replace(column,'([^A-Za-z0-9\\s+])', ''))).alias('words')\n",
    "\n",
    "# Cleanup function\n",
    "def clean_up(rdd_words):\n",
    "  import re # Import regex library\n",
    "  rdd_words_clean1 = re.sub(r'(03:019:024)', '', rdd_words) # certain verse\n",
    "  rdd_words_clean2 = re.sub(r'([^A-Za-z0-9\\s+])', '', rdd_words_clean1) # Nonwords  \n",
    "  rdd_words_split = rdd_words_clean2.split(' ') # Split data\n",
    "  return [word.lower() for word in rdd_words_split if word != ''] # Lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split          # Function to split data\n",
    "from pyspark.sql.functions import explode        # Equivalent to flatMap\n",
    "\n",
    "# Create Session\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "                    .appName(\"df\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_ulysses = spark.read.text(\"4300-2.txt\")\n",
    "df_bible = spark.read.text(\"bible.txt\")\n",
    "df_stopwords = spark.read.text(\"stopwords.csv\")\n",
    "\n",
    "# Select words\n",
    "df_ulysses_all = df_ulysses.select(split(df_ulysses.value, \" \").alias(\"words\"))\n",
    "df_ulysses_all = df_ulysses_all.select(explode(df_ulysses_all.words).alias(\"words\"))\n",
    "df_ulysses_all = df_ulysses_all.select(removePunctuation(col('words')))\n",
    "df_ulysses_all = df_ulysses_all.filter('words != Null or words != \"\"')\n",
    "\n",
    "df_bible_all = df_bible.select(split(df_bible.value, \" \").alias(\"words\"))\n",
    "df_bible_all = df_bible_all.select(explode(df_bible_all.words).alias(\"words\"))\n",
    "df_bible_all = df_bible_all.select(removePunctuation(col('words')))\n",
    "df_bible_all = df_bible_all.filter('words != Null or words != \"\"')\n",
    "\n",
    "# Remove stopwords\n",
    "df_ulysses_cleaned = df_ulysses_all.join(df_stopwords, df_ulysses_all.words \n",
    "                                         == df_stopwords.value, \\\n",
    "                                         'left_anti').select(df_ulysses_all.words)\n",
    "df_bible_cleaned = df_bible_all.join(df_stopwords, df_bible_all.words\n",
    "                                     == df_stopwords.value, \\\n",
    "                                     'left_anti').select(df_bible_all.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most frequent words: \n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "|  the|45043|\n",
      "|   of|24576|\n",
      "|  and|21758|\n",
      "|    a|19569|\n",
      "|   to|15095|\n",
      "|   in|14831|\n",
      "|   he|12080|\n",
      "|  his| 9983|\n",
      "|    i| 8093|\n",
      "| that| 7846|\n",
      "| with| 7565|\n",
      "|   it| 7131|\n",
      "|  was| 6400|\n",
      "|   on| 6361|\n",
      "|  for| 5866|\n",
      "|  you| 5842|\n",
      "|  her| 5353|\n",
      "|  him| 4570|\n",
      "|   is| 4369|\n",
      "|  all| 3988|\n",
      "|   by| 3901|\n",
      "|   at| 3890|\n",
      "| said| 3617|\n",
      "|   as| 3605|\n",
      "|  she| 3402|\n",
      "| from| 3290|\n",
      "| they| 3074|\n",
      "|   or| 3054|\n",
      "|   me| 2824|\n",
      "|bloom| 2798|\n",
      "+-----+-----+\n",
      "only showing top 30 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get frequent word pair\n",
    "df_ulysses_unique = df_ulysses_cleaned.groupBy(\"words\").count()\n",
    "df_ulysses_unique = df_ulysses_unique.orderBy([\"count\"], ascending=False)\n",
    "print(\"30 Most frequent words: \")\n",
    "print(df_ulysses_unique.show(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Most frequent words: \n",
      "+-----+-----+\n",
      "|words|count|\n",
      "+-----+-----+\n",
      "|  the|64294|\n",
      "|  and|51836|\n",
      "|   of|34868|\n",
      "|   to|13722|\n",
      "| that|12943|\n",
      "|   in|12785|\n",
      "|   he|10424|\n",
      "|shall| 9842|\n",
      "|  for| 9023|\n",
      "| unto| 8997|\n",
      "|    i| 8854|\n",
      "|  his| 8473|\n",
      "|    a| 8291|\n",
      "| lord| 7830|\n",
      "| they| 7382|\n",
      "|   be| 7051|\n",
      "|   is| 7041|\n",
      "|  him| 6659|\n",
      "|  not| 6638|\n",
      "| them| 6430|\n",
      "|   it| 6159|\n",
      "| with| 6110|\n",
      "|  all| 5656|\n",
      "| thou| 5474|\n",
      "|  thy| 4600|\n",
      "|  was| 4524|\n",
      "|  god| 4443|\n",
      "|which| 4427|\n",
      "|   my| 4368|\n",
      "|   me| 4096|\n",
      "+-----+-----+\n",
      "only showing top 30 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get frequent word pair\n",
    "df_bible_unique = df_bible_cleaned.groupBy(\"words\").count()\n",
    "df_bible_unique = df_bible_unique.orderBy([\"count\"], ascending=False)\n",
    "print(\"30 Most frequent words: \")\n",
    "print(df_bible_unique.show(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ii) Get unique words\n",
    "Create DF-s that contain only words unique for each of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30038"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ulysess_dist = df_ulysses_all.distinct()\n",
    "df_ulysess_dist.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44285"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bible_dist = df_bible_all.distinct()\n",
    "df_bible_dist.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Get common words\n",
    "Finally create an DF that contains only the words common to both texts. In latest DF preserve numbers of occurrences in two texts. In other words a row in your DF will look like (love 45 32). Print or store the words and the numbers of occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+-----------+\n",
      "|words_ulysses|count_ulysses|words_bible|count_bible|\n",
      "+-------------+-------------+-----------+-----------+\n",
      "|          the|        45043|        the|      64294|\n",
      "|           of|        24576|         of|      34868|\n",
      "|          and|        21758|        and|      51836|\n",
      "|            a|        19569|          a|       8291|\n",
      "|           to|        15095|         to|      13722|\n",
      "+-------------+-------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined = df_ulysses_unique.join(df_bible_unique, \\\n",
    "                                     df_ulysses_unique.words\\\n",
    "                                     == df_bible_unique.words, 'inner')\n",
    "df_combined = df_combined.toDF(\"words_ulysses\", \"count_ulysses\", \\\n",
    "                               \"words_bible\", \"count_bible\")\n",
    "df_combined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) 20 most frequent words\n",
    "Create for us the list of 20 most frequently used words common to both texts. In your report, print (store) the words, followed by the number of occurrences in Ulysses and then the Bible. Order your report in descending order starting by the number of occurrences in Ulysses. Present the same data this time ordered by the number of occurrences in the Bible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined = df_combined.select(['words_ulysses', 'count_ulysses',\n",
    "                                  'count_bible'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6097\n"
     ]
    }
   ],
   "source": [
    "print(df_combined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+\n",
      "|words_ulysses|count_ulysses|count_bible|\n",
      "+-------------+-------------+-----------+\n",
      "|          the|        45043|      64294|\n",
      "|           of|        24576|      34868|\n",
      "|          and|        21758|      51836|\n",
      "|            a|        19569|       8291|\n",
      "|           to|        15095|      13722|\n",
      "|           in|        14831|      12785|\n",
      "|           he|        12080|      10424|\n",
      "|          his|         9983|       8473|\n",
      "|            i|         8093|       8854|\n",
      "|         that|         7846|      12943|\n",
      "|         with|         7565|       6110|\n",
      "|           it|         7131|       6159|\n",
      "|          was|         6400|       4524|\n",
      "|           on|         6361|       2033|\n",
      "|          for|         5866|       9023|\n",
      "|          you|         5842|       2752|\n",
      "|          her|         5353|       1994|\n",
      "|          him|         4570|       6659|\n",
      "|           is|         4369|       7041|\n",
      "|          all|         3988|       5656|\n",
      "+-------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.orderBy(col('count_ulysses').desc()).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+\n",
      "|words_ulysses|count_ulysses|count_bible|\n",
      "+-------------+-------------+-----------+\n",
      "|          the|        45043|      64294|\n",
      "|          and|        21758|      51836|\n",
      "|           of|        24576|      34868|\n",
      "|           to|        15095|      13722|\n",
      "|         that|         7846|      12943|\n",
      "|           in|        14831|      12785|\n",
      "|           he|        12080|      10424|\n",
      "|        shall|          198|       9842|\n",
      "|          for|         5866|       9023|\n",
      "|         unto|           15|       8997|\n",
      "|            i|         8093|       8854|\n",
      "|          his|         9983|       8473|\n",
      "|            a|        19569|       8291|\n",
      "|         lord|          447|       7830|\n",
      "|         they|         3074|       7382|\n",
      "|           be|         2697|       7051|\n",
      "|           is|         4369|       7041|\n",
      "|          him|         4570|       6659|\n",
      "|          not|         2726|       6638|\n",
      "|         them|         2025|       6430|\n",
      "+-------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.orderBy(col('count_bible').desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v) Get a random sample\n",
    "List for us a random samples containing 5% of words in the final DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+\n",
      "|words_ulysses|count_ulysses|count_bible|\n",
      "+-------------+-------------+-----------+\n",
      "|           to|        15095|      13722|\n",
      "|           in|        14831|      12785|\n",
      "|           he|        12080|      10424|\n",
      "|         that|         7846|      12943|\n",
      "|           it|         7131|       6159|\n",
      "|          was|         6400|       4524|\n",
      "|           on|         6361|       2033|\n",
      "|          for|         5866|       9023|\n",
      "|           is|         4369|       7041|\n",
      "|          all|         3988|       5656|\n",
      "|           at|         3890|       1600|\n",
      "|         said|         3617|       3999|\n",
      "|         from|         3290|       3676|\n",
      "|         they|         3074|       7382|\n",
      "|          out|         2700|       2777|\n",
      "|           be|         2697|       7051|\n",
      "|           my|         2511|       4368|\n",
      "|           up|         2495|       2386|\n",
      "|        their|         2157|       3932|\n",
      "|        there|         2117|       2303|\n",
      "+-------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# List for us a random samples containing 5% of words in the final RDD.\n",
    "final_df_sample = df_combined.sample(False, 0.5, 123)\n",
    "print(final_df_sample.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3 (30%)\n",
    "Consider attached files *transactions.txt* and *products.txt*.\n",
    "\n",
    "### i) Load data\n",
    "Each line in *transactions.txt* file contains a *transaction date*, *time*, *customer id*, *product id*, *quantity bought* and *price paid*, delimited with hash (#) sign. Each line in file *products.txt* contains *product id*, *product name*, *unit price* and *quantity available* in the store. Bring those data in Spark and organize it as DataFrames with named columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_transactions = spark.read.csv(\"transactions.txt\", sep=\"#\")\n",
    "df_products = spark.read.csv(\"products.txt\", sep=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_transactions = df_transactions.withColumnRenamed('_c0', \"transaction_date\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c1', \"time\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c2', \"customer_id\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c3', \"product_id\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c4', \"quantity_bought\")\n",
    "df_transactions = df_transactions.withColumnRenamed('_c5', \"price_paid\")\n",
    "\n",
    "df_products = df_products.withColumnRenamed('_c0', \"product_id\")\n",
    "df_products = df_products.withColumnRenamed('_c1', \"product_name\")\n",
    "df_products = df_products.withColumnRenamed('_c2', \"unit_price\")\n",
    "df_products = df_products.withColumnRenamed('_c3', \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Largest spending\n",
    "Using either DataFrame methods or plain SQL statements find 5 customers with the largest spent on the day. Find the names of the products each of those 5 customers bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_transactions.groupBy(\"customer id\").sum().show()\n",
    "df_cust_spend = df_transactions.groupBy('customer_id', \\\n",
    "                                        'transaction_date')\n",
    "                                .agg({'price_paid': 'sum'})\n",
    "df_cust_spend = df_cust_spend.orderBy('sum(price_paid)', \\\n",
    "                                      ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tables\n",
    "df_cust_spend.createOrReplaceTempView(\"tbl_cust_spend\")\n",
    "df_transactions.createOrReplaceTempView(\"tbl_transactions\")\n",
    "df_products.createOrReplaceTempView(\"tbl_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------------+\n",
      "|customer_id|transaction_date|   sum(price_paid)|\n",
      "+-----------+----------------+------------------+\n",
      "|         76|      2015-03-30|100049.00000000001|\n",
      "|         53|      2015-03-30| 88829.76000000001|\n",
      "|         56|      2015-03-30|          85906.94|\n",
      "|         51|      2015-03-30|          83312.12|\n",
      "|         31|      2015-03-30|          83202.61|\n",
      "+-----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top5 = spark.sql(\"SELECT * FROM tbl_cust_spend LIMIT 5\")\n",
    "df_top5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-----------+----------+---------------+----------+\n",
      "|transaction_date|    time|customer_id|product_id|quantity_bought|price_paid|\n",
      "+----------------+--------+-----------+----------+---------------+----------+\n",
      "|      2015-03-30| 6:55 AM|         51|        68|              1|   9506.21|\n",
      "|      2015-03-30| 7:39 PM|         99|        86|              5|   4107.59|\n",
      "|      2015-03-30|11:57 AM|         79|        58|              7|   2987.22|\n",
      "|      2015-03-30|12:46 AM|         51|        50|              6|   7501.89|\n",
      "|      2015-03-30|11:39 AM|         86|        24|              5|    8370.2|\n",
      "+----------------+--------+-----------+----------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|        product_name|\n",
      "+-----------+--------------------+\n",
      "|          1|SAMSUNG LED TV 42...|\n",
      "|          1|ROBITUSSIN PEAK C...|\n",
      "|          1|    LEGO Minifigures|\n",
      "|          1|           Glipizide|\n",
      "|          1|Scrub Care Povido...|\n",
      "|          1|Medal Of Honor Al...|\n",
      "|          1|Notebook Lenovo U...|\n",
      "|          1|        LEGO Technic|\n",
      "|          1|PC HP 490PD MT, D...|\n",
      "|         10|              Ativan|\n",
      "|         10|   LEGO Galaxy Squad|\n",
      "|         10|SAMSUNG LED TV 32...|\n",
      "|         10|          Dictionary|\n",
      "|         10|ROBITUSSIN PEAK C...|\n",
      "|         10|Procesor Intel Co...|\n",
      "|         10|GAM X360 Hitman A...|\n",
      "|        100|    chest congestion|\n",
      "|        100|PC HP 490PD MT, D...|\n",
      "|        100|     LEGO The Hobbit|\n",
      "|        100|Roller Derby Roll...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top5_products = df_transactions.join(df_top5, \\\n",
    "                                        df_transactions.customer_id \\\n",
    "                                        == df_top5.customer_id, \"left\")\\\n",
    "                                  .select(df_transactions.customer_id, \\\n",
    "                                          df_transactions.product_id)\n",
    "df_top5_list = df_top5_products.join(df_products, df_top5_products.product_id \\\n",
    "                                     == df_products.product_id, \"left\")\\\n",
    "                                        .select(df_top5_products.customer_id,\n",
    "                                                df_products.product_name)\n",
    "df_top5_list.orderBy(\"customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Total number sold\n",
    "Find the names and total number sold of 10 most popular products. Order products once per the number sold and then by the total value (quanity*price) sold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+------------------+\n",
      "|        product_name|quantity|unit_price|       Total value|\n",
      "+--------------------+--------+----------+------------------+\n",
      "|Notebook Lenovo U...|   226.0|    461.08|         104204.08|\n",
      "|SAMSUNG LED TV 39...|   142.0|   2531.15|          359423.3|\n",
      "|               Jafra|   102.0|   3715.07|         378937.14|\n",
      "|            Jantoven|   102.0|    3255.4|          332050.8|\n",
      "|Far Cry 4 Limited...|   101.0|    711.88|          71899.88|\n",
      "|Roller Derby Roll...|    91.0|   7783.79|         708324.89|\n",
      "|Procesor Intel Co...|    90.0|   4570.99|          411389.1|\n",
      "|  Sony Playstation 3|    88.0|   5088.35|447774.80000000005|\n",
      "|    chest congestion|    84.0|   1305.04|         109623.36|\n",
      "|Barbie Beach Ken ...|    82.0|    742.84|60912.880000000005|\n",
      "+--------------------+--------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List the sum of sold products\n",
    "df_sum_products=df_transactions.groupBy('product_id')\n",
    "                               .agg({'quantity_bought': 'sum'})\n",
    "df_sum_products = df_sum_products\n",
    "                                .orderBy('sum(quantity_bought)', ascending=False)\n",
    "\n",
    "# Get top ten results\n",
    "df_sum_products.createOrReplaceTempView(\"tbl_sum_products\")\n",
    "df_top10_products = spark.sql(\"SELECT * FROM tbl_sum_products LIMIT 10\")\n",
    "\n",
    "# Calculate the total value\n",
    "df_products_distinct = df_products.select(df_products.product_id,\n",
    "                                        df_products.product_name,\n",
    "                                          df_products.unit_price).distinct()\n",
    "df_top10_products = df_top10_products.join(df_products_distinct,\n",
    "                                           df_top10_products.product_id \n",
    "                                           == df_products_distinct.product_id, \"left\")\n",
    "df_top10_products = df_top10_products.select(df_top10_products['product_name'],\n",
    "                                             df_top10_products['sum(quantity_bought)']\n",
    "                                             .alias(\"quantity\"),\n",
    "                                             df_top10_products['unit_price'],\n",
    "                                             (df_top10_products['sum(quantity_bought)'] * \n",
    "                                              df_top10_products['unit_price'])\n",
    "                                             .alias(\"Total value\"))\n",
    "df_top10_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20%)\n",
    "Implement problem 3 using RDD APIs.\n",
    "\n",
    "### i) Load data\n",
    "Each line in *transactions.txt* file contains a *transaction date*, *time*, *customer id*, *product id*, *quantity bought* and *price paid*, delimited with hash (#) sign. Each line in file *products.txt* contains *product id*, *product name*, *unit price* and *quantity available* in the store. Bring those data in Spark and organize it as DataFrames with named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "rdd_transactions = sc.textFile(\"transactions.txt\")\n",
    "rdd_transactions = rdd_transactions.map(lambda x: x.split(\"#\"))\n",
    "rdd_transactions = rdd_transactions.map(lambda x: Row(transaction_date = x[0],\n",
    "                                                      time = x[1],\n",
    "                                                      customer_id = int(x[2]),\n",
    "                                                      product_id = int(x[3]),\n",
    "                                                      quantity_bought = int(x[4]),\n",
    "                                                      price_paid = float(x[5])))\n",
    "\n",
    "rdd_products = sc.textFile(\"products.txt\")\n",
    "rdd_products = rdd_products.map(lambda x: x.split(\"#\"))\n",
    "rdd_products = rdd_products.map(lambda x: Row(product_id = int(x[0]),\n",
    "                                              product_name = x[1],\n",
    "                                              unit_price = float(x[2]),\n",
    "                                              quantity = int(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_id=51, price_paid=9506.21, product_id=68, quantity_bought=1, time='6:55 AM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=99, price_paid=4107.59, product_id=86, quantity_bought=5, time='7:39 PM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=79, price_paid=2987.22, product_id=58, quantity_bought=7, time='11:57 AM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=51, price_paid=7501.89, product_id=50, quantity_bought=6, time='12:46 AM', transaction_date='2015-03-30'),\n",
       " Row(customer_id=86, price_paid=8370.2, product_id=24, quantity_bought=5, time='11:39 AM', transaction_date='2015-03-30')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_transactions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product_id=1, product_name='ROBITUSSIN PEAK COLD NIGHTTIME COLD PLUS FLU', quantity=10, unit_price=9721.89),\n",
       " Row(product_id=2, product_name='Mattel Little Mommy Doctor Doll', quantity=6, unit_price=6060.78),\n",
       " Row(product_id=3, product_name='Cute baby doll, battery', quantity=2, unit_price=1808.79),\n",
       " Row(product_id=4, product_name='Bear doll', quantity=6, unit_price=51.06),\n",
       " Row(product_id=5, product_name='LEGO Legends of Chima', quantity=6, unit_price=849.36)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_products.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Largest spending\n",
    "Using either RDD methods or plain SQL statements find 5 customers with the largest spent on the day. Find the names of the products each of those 5 customers bought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SQL schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString1 = \"transaction_date time customer_id product_id quantity_bought price_paid\"\n",
    "schemaString2 = \"product_id time product_name unit_price quantity\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString1.split()]\n",
    "schema1 = StructType(fields)\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString2.split()]\n",
    "schema2 = StructType(fields)\n",
    "\n",
    "# Create schema\n",
    "sch_transactions = spark.createDataFrame(rdd_transactions, schema)\n",
    "sch_products = spark.createDataFrame(rdd_products, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "sch_transactions.createOrReplaceTempView(\"tbl_transactions\")\n",
    "sch_products.createOrReplaceTempView(\"tbl_products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The SQL group by method somehow didn't worrk. That is why the DF method was used. In order to solve the problem the following SQL stament should work:\n",
    "\n",
    "```sql\n",
    "SELECT customer_id, SUM(to_float(quantity_bought) * to_float(price_paid)) AS revenue\n",
    "FROM tbl_transactions\n",
    "GROUP BY customer_id\n",
    "ORDER BY revenue DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sch_transactions = df_transactions.groupBy('customer_id', 'transaction_date').agg({'price_paid': 'sum'})\n",
    "sch_transactions = sch_transactions.orderBy('sum(price_paid)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_id=76, transaction_date='2015-03-30', sum(price_paid)=100049.00000000001),\n",
       " Row(customer_id=53, transaction_date='2015-03-30', sum(price_paid)=88829.76000000001),\n",
       " Row(customer_id=56, transaction_date='2015-03-30', sum(price_paid)=85906.94),\n",
       " Row(customer_id=51, transaction_date='2015-03-30', sum(price_paid)=83312.12),\n",
       " Row(customer_id=31, transaction_date='2015-03-30', sum(price_paid)=83202.61)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sch_transactions.createOrReplaceTempView(\"tbl_transactions2\")\n",
    "tbl_cust_spend = spark.sql(\"SELECT * FROM tbl_transactions2 ORDER BY 'sum(price_paid)' DESC\")\n",
    "tbl_cust_spend.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the code below doesn't run. However, it would be the necessary SQL command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 't.product_id' is ambiguous, could be: product_id#1616, product_id#1679.; line 1 pos 74\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o526.sql.\n: org.apache.spark.sql.AnalysisException: Reference 't.product_id' is ambiguous, could be: product_id#1616, product_id#1679.; line 1 pos 74\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:287)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$6$$anonfun$37.apply(Analyzer.scala:851)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$6$$anonfun$37.apply(Analyzer.scala:851)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$6.applyOrElse(Analyzer.scala:851)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$6.applyOrElse(Analyzer.scala:848)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:290)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:848)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:790)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:790)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:668)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-616c74ab478a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_top5_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM tbl_transactions t LEFT JOIN tbl_products p ON t.product_id == p.product_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_top5_products\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tbl_top5_products\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_top5_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT t.customer_id FROM tbl_products p LEFT JOIN tbl_top5_products t ON t.product_id == p.product_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_top5_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_top5_list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_top5_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM tbl_cust_spend ORDER BY 'customer_id' DESC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 't.product_id' is ambiguous, could be: product_id#1616, product_id#1679.; line 1 pos 74\""
     ]
    }
   ],
   "source": [
    "df_top5_products = spark.sql(\"SELECT * FROM tbl_transactions t \\\n",
    "                              LEFT JOIN tbl_products p ON \\\n",
    "                              t.product_id == p.product_id\")\n",
    "df_top5_products.createOrReplaceTempView(\"tbl_top5_products\")\n",
    "df_top5_list = spark.sql(\"SELECT t.customer_id FROM tbl_products p \\\n",
    "                          LEFT JOIN tbl_top5_products t ON \\\n",
    "                         t.product_id == p.product_id\")\n",
    "df_top5_list.createOrReplaceTempView(\"df_top5_list\")\n",
    "df_top5_list = spark.sql(\"SELECT * FROM tbl_cust_spend ORDER BY \\\n",
    "                         'customer_id' DESC\")\n",
    "df_top5_list.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Total number sold\n",
    "Find the names and total number sold of 10 most popular products. Order products once per the number sold and then by the total value (quanity*price) sold. \n",
    "\n",
    "**Note**: It's the same problem as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+------------------+\n",
      "|        product_name|quantity|unit_price|       Total value|\n",
      "+--------------------+--------+----------+------------------+\n",
      "|Notebook Lenovo U...|     226|    461.08|         104204.08|\n",
      "|SAMSUNG LED TV 39...|     142|   2531.15|          359423.3|\n",
      "|            Jantoven|     102|    3255.4|          332050.8|\n",
      "|               Jafra|     102|   3715.07|         378937.14|\n",
      "|Far Cry 4 Limited...|     101|    711.88|          71899.88|\n",
      "|Roller Derby Roll...|      91|   7783.79|         708324.89|\n",
      "|Procesor Intel Co...|      90|   4570.99|          411389.1|\n",
      "|  Sony Playstation 3|      88|   5088.35|447774.80000000005|\n",
      "|    chest congestion|      84|   1305.04|         109623.36|\n",
      "|Barbie Beach Ken ...|      82|    742.84|60912.880000000005|\n",
      "+--------------------+--------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List the sum of sold products\n",
    "df_sum_products=df_transactions.groupBy('product_id').agg({'quantity_bought': 'sum'})\n",
    "df_sum_products = df_sum_products.orderBy('sum(quantity_bought)',\n",
    "                                          ascending=False)\n",
    "\n",
    "# Get top ten results\n",
    "df_sum_products.createOrReplaceTempView(\"tbl_sum_products\")\n",
    "df_top10_products = spark.sql(\"SELECT * FROM tbl_sum_products LIMIT 10\")\n",
    "\n",
    "# Calculate the total value\n",
    "df_products_distinct = df_products.select(df_products.product_id,\n",
    "                                          df_products.product_name,\n",
    "                                          df_products.unit_price).distinct()\n",
    "df_top10_products = df_top10_products.join(df_products_distinct,\n",
    "                                           df_top10_products.product_id\n",
    "                                           == df_products_distinct.product_id, \"left\")\n",
    "df_top10_products = df_top10_products.select(df_top10_products['product_name'],\n",
    "                                             df_top10_products['sum(quantity_bought)']\n",
    "                                             .alias(\"quantity\"),\n",
    "                                             df_top10_products['unit_price'],\n",
    "                                             (df_top10_products['sum(quantity_bought)'] *\n",
    "                                              df_top10_products['unit_price'])\n",
    "                                             .alias(\"Total value\"))\n",
    "df_top10_products.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
