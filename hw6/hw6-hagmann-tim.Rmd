---
title: |
  | Homework 6: Spark Streaming
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
linkcolor: blue
subtitle: |
  | E-63 Big Data Analytics
  | Harvard University, Autumn 2017
affiliation: Harvard University
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```

## Problem 1 (20%)
Lecture notes contain script *network-count.py* in both Spark Streaming APIand Spark Structured Streaming API. Use Linux *nc (NetCat)* utility to demonstrate that scripts work. Run both scripts on your own VM with Spark 2.2 installation. Cloudera VM with Spark 1.6 does not have Spark Structured Streaming API.

```{r}
"/home/tim/e63-coursework/hw6/scripts/network-count.py"
```

```{bash eval=FALSE}
python /home/tim/e63-coursework/hw6/scripts/network-count.py localhost 9999
```

```text
Usage: network_wordcount.py <hostname> <port>
```

```{bash eval=FALSE}

nc -lk 9999 

#in one window. In another, run the .py program.
#to doble-check everything works, aka text is available via socket, run, 
#in other window, <nc localhost 9999>. Change port to a different number if 9999 is taken 
cd ~/hw5_data
spark-submit --master local[4] network-count.py localhost 9999

Ignore the SLF4J messages.
They do not interfere with running our spark jobs. They basically tell that zookeeper and some other services are using spark or depend on spark or spark depends on them. Something like that.

Type something in the terminal where you run nc -lk 9999; you should see results being computed in the window where network-count.py is running. 
Exit when you're tired of playing with it and finally thirsty for running more scripts. :)
```


## Problem 2 (30%)
Expand provide *orders.tar.gz* file. Also, download shell scrips *splitAndSend.original.sh* and *splitAndSend.sh* and the Python script *count-buys.py*. First run *splitAndSend.original.sh* and *count-buys.py*. Record the failure mode of *count-buys.py*. Simply read the error message produced and tell us what is happening. Then run script *splitAndSend.sh* and Python program *count-buys.py* and tell us what the results are. In both cases show use contents of your HDFS directories input, output and staging. The second script *splitAndSend.sh* is supposed to reduce or eliminate the race condition.  You might want to rename HDFS directory output from the first run in order to preserve itâ€™s content. In both cases, show the partial contents of your HDFS directories *input*, *output* and *staging*. In the second run, locate an output file named *part-00000* that is not empty and show its content to us.  Run these experiments on Cloudera VM. You need HDFS for these programs to run.

## Problem 3 (10%)
In the second run of the previous problem you will notice that many of part-00000 files in your output directory are empty. Could you explain why.
(10%)

## Problem 4 (20%)
Could you rewrite count-buys.sh in Spark Structured Streaming API. If you do that change script splitAndSend.sh to move generated chunks from the local files system directory staging to local file system directory input. Run this experiment on your VM with Spark 2.2.

## Problem 5 (20%)
Examine provided Python program stateful_wordcount.py. Make it work as is. If there are errors on the code, fix them. Modify the code so that it outputs the number of words starting with letters a and b. Demonstrate that modified program work. You should provide several both positive and negative examples.
