---
title: |
  | Homework 6: Spark Streaming
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
linkcolor: blue
subtitle: |
  | E-63 Big Data Analytics
  | Harvard University, Autumn 2017
affiliation: Harvard University
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```


## Problem 1 (20%)
Lecture notes contain script *network-count.py* in both Spark Streaming API and Spark Structured Streaming API. Use Linux *nc (NetCat)* utility to demonstrate that scripts work. Run both scripts on your own VM with Spark 2.2 installation. Cloudera VM with Spark 1.6 does not have Spark Structured Streaming API.

**Note:** We're running Problem 1, 4 and 5 on the same EC2 instance as in HW3 (See HW3 for the setup instructions). Problem 2 and 3 are run on the cloudera quickstart docker images. The setup instructions can be found in P2. 

### i) Spark Streaming API
First we're starting spark. 

```{bash eval=FALSE}
sudo spark/sbin/start-all.sh
```
```text
starting org.apache.spark.deploy.master.Master, logging to /home/tim/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-ip-172-31-24-35.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/tim/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ip-172-31-24-35.out
```
Next we're running the *network-count.py* in one window and the *nc -lk 9999* and typing *abc* in another window gives the following result:

```{bash eval=FALSE}
nc -lk 9999 
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/network-count.py localhost 9999
```

#### Netcat server
In the *NetCat* server we can input again the following text: 
```text
tim@ip-172-31-24-35:~/e63-coursework/hw6/data$ nc -lk 9999
hello world

this is a test
talking to the world
```

#### Network count
The network count produces with the above input the following output:
```text
-------------------------------------------
Time: 2017-10-13 14:46:45
-------------------------------------------
(u'world', 1)
(u'hello', 1)

-------------------------------------------
Time: 2017-10-13 14:46:48
-------------------------------------------

-------------------------------------------
Time: 2017-10-13 14:46:51
-------------------------------------------
(u'a', 2)
(u'this', 1)
(u'test', 1)
(u'to', 1)
(u'world', 2)
(u'talking', 1)
(u'the', 2)
(u'is', 1)
...

-------
```
As can be seen above, the *network-count.py* script appears to be working. It correctly counts the *hello world* line as well as the other two lines.

### ii) Spark Structured Streaming API
In order to run on the *Spark Structured Streaming API* we have to use the *network-count-structured.py*. The *NetCat* server is still running, that means we don't have to start it again. 
```{bash eval=FALSE}
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/network-count-structured.py localhost 9999
```

#### Netcat server
In the *NetCat* server we can input again the same text as seen above. 
```text
tim@ip-172-31-24-35:~/e63-coursework/hw6/data$ nc -lk 9999
hello world

this is a test
talking to the world
```

#### Network count structured
The network count produces with the above input the following output:
```text
tim@ip-172-31-24-35:~/e63-coursework$ spark-submit --master local[4] /home/tim/e63-coursework/hw6/script 
s/network-count-structured.py localhost 9999
-------------------------------------------
Batch: 0
-------------------------------------------

[Stage 1:====>                                                   (15 + 4) / 200]
[Stage 1:======>                                                 (22 + 5) / 200]
[Stage 1:========>                                               (30 + 4) / 200]
[Stage 1:===========>                                            (40 + 4) / 200]
[Stage 1:==============>                                         (50 + 4) / 200]
[Stage 1:=================>                                      (62 + 4) / 200]
[Stage 1:====================>                                   (74 + 4) / 200]
[Stage 1:=======================>                                (84 + 4) / 200]
[Stage 1:===========================>                            (97 + 4) / 200]
[Stage 1:==============================>                        (112 + 4) / 200]
[Stage 1:=================================>                     (123 + 4) / 200]
[Stage 1:=====================================>                 (137 + 4) / 200]
[Stage 1:=========================================>             (151 + 4) / 200]
[Stage 1:=============================================>         (164 + 4) / 200]
[Stage 1:================================================>      (178 + 4) / 200]
[Stage 1:====================================================>  (191 + 4) / 200]
                                                                                
+-----+-----+
| word|count|
+-----+-----+
|hello|    1|
|world|    1|
+-----+-----+

-------------------------------------------
Batch: 1
-------------------------------------------

[Stage 5:===========>                                            (41 + 4) / 200]
[Stage 5:===============>                                        (54 + 4) / 200]
[Stage 5:===================>                                    (70 + 4) / 200]
[Stage 5:========================>                               (86 + 4) / 200]
[Stage 5:============================>                          (102 + 4) / 200]
[Stage 5:================================>                      (117 + 4) / 200]
[Stage 5:====================================>                  (133 + 4) / 200]
[Stage 5:=======================================>               (145 + 4) / 200]
[Stage 5:============================================>          (161 + 4) / 200]
[Stage 5:================================================>      (176 + 4) / 200]
[Stage 5:===================================================>   (189 + 4) / 200]
                                                                                
+-----+-----+
| word|count|
+-----+-----+
|hello|    1|
|   is|    1|
|world|    1|
|    a|    1|
| this|    1|
| test|    1|
+-----+-----+

-------------------------------------------
Batch: 2
-------------------------------------------

[Stage 9:============>                                           (46 + 4) / 200]
[Stage 9:=================>                                      (62 + 4) / 200]
[Stage 9:======================>                                 (79 + 4) / 200]
[Stage 9:==========================>                             (94 + 4) / 200]
[Stage 9:=============================>                         (107 + 4) / 200]
[Stage 9:=================================>                     (120 + 4) / 200]
[Stage 9:=====================================>                 (138 + 4) / 200]
[Stage 9:==========================================>            (153 + 4) / 200]
[Stage 9:==============================================>        (170 + 4) / 200]
[Stage 9:===================================================>   (186 + 4) / 200]
                                                                                
+-------+-----+
|   word|count|
+-------+-----+
|  hello|    1|
|talking|    1|
|     is|    1|
|    the|    1|
|  world|    2|
|      a|    1|
|   this|    1|
|   test|    1|
|     to|    1|
+-------+-----+
```

The *network-count-structured.py* is also working. It is running in batches and splits the analysis before giving the final output in *batch 2*.

## Problem 2 (30%)
Run the following experiments on Cloudera VM. You need HDFS for these programs to run.

### i) WSL Setup
A lot of company notebook don't allow running virtual machines (VT-X is disabled). This is also the case in my case. An alternative of using VM is to use the docker image. After installing docker nativily, it is possible to run docker against the WSL engine. This is because Docker can expose a TCP endpoint which the CLI (i.e., WSL) can attach to.

**Note:** The TCP endpoint on Windows is turned off by default. To activate it, right-click the Docker icon in your taskbar and choose Settings, and tick the box next to "Expose daemon on tcp://localhost:2375 without TLS".

#### Add docker to WSL
With that done, all we need to do is instruct the CLI under Bash to connect to the engine running under Windows instead of to the non-existing engine running under Bash, like this:

```{bash eval=FALSE}
echo "export DOCKER_HOST='tcp://0.0.0.0:2375'" >> ~/.bashrc
source ~/.bashrc
```

#### Install CDH 5.12
```{bash eval=FALSE}
docker pull cloudera/quickstart:latest
```

### ii) Start docker
```{bash eval=FALSE}
docker run --hostname=quickstart.cloudera --privileged=true -it --rm -i -t -p 8888:8888 -p 80:80 -p 7180:7180 -p 8080:8080 -p 18080:18080 -p 18081:18081 -p 8020:8020 cloudera/quickstart /usr/bin/docker-quickstart
```

#### Start second docker
Starting a secon terminal it is possible to login to docker from it. In order to do this we first need the container image id.
```{bash eval=FALSE}
# Get id name
docker ps
```

```text
CONTAINER ID        IMAGE                 COMMAND                  CREATED
819a984e686f        cloudera/quickstart   "/usr/bin/docker-q..."   2 minutes ago

STATUS              PORTS
Up 2 minutes        0.0.0.0:80->80/tcp, 0.0.0.0:7180->7180/tcp, 0.0.0.0:8080->8080/tcp, ...
```

With the above information we an login to docker.
```{bash eval=FALSE}
# Login with a second screen
docker exec -it --user root 4bff53893b88 bash
```

#### Change to cloudera user
The docker image starts by default into the root user. We're changing to the installed cloudera user (password=cloudera). 
```{bash eval=FALSE}
su cloudera
cd ~
```

### iii) Setup Image
```{bash eval=FALSE}
# Add environment variables
echo "export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera" >> /home/cloudera/.bash_profile
echo "export SPARK_HOME=/usr/lib/spark" >> /home/cloudera/.bash_profile
echo "export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce" >> /home/cloudera/.bash_profile
echo "export PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$SPARK_HOME/bin" >> /home/cloudera/.bash_profile
source /home/cloudera/.bash_profile
```

```{bash eval=FALSE}
sudo yum install python-pip
sudo pip install --upgrade pip
sudo pip install findspark
```

#### Modify log4j.properties
In order to eliminate the error messages in the **spark-shell**, we have to create the log4j.properties file from the template file and setting the log level to ERROR and log4j.rootCategory=ERROR.

```{bash eval=FALSE}
# Create log4j.properties
sudo mv /usr/lib/spark/conf/log4j.properties.template /usr/lib/spark/conf/log4j.properties

# Edit file
sudo vi /usr/lib/spark/conf/log4j.properties

# Replace INFO, WARN with ERROR.
:g /WARN/s//ERROR/g
:g /INFO/s//ERROR/g
:wq
```

#### Start spark
```{bash eval=FALSE}
sudo /usr/lib/spark/sbin/start-all.sh
```

#### Check status
```{bash eval=FALSE}
for x in `cd /etc/init.d ; ls hadoop-*`; do sudo service $x status ; done
```

```text
[root@quickstart /]# for x in `cd /etc/init.d ; ls hadoop-*`; do sudo service $x status ; done
Hadoop datanode is running                                 [  OK  ]
Hadoop journalnode is running                              [  OK  ]
Hadoop namenode is running                                 [  OK  ]
Hadoop secondarynamenode is running                        [  OK  ]
Hadoop httpfs is running                                   [  OK  ]
Hadoop historyserver is running                            [  OK  ]
Hadoop nodemanager is running                              [  OK  ]
Hadoop proxyserver is not running                          [FAILED]
Hadoop resourcemanager is running                          [  OK  ]
```
The above output shows, that all the necessary hadoop services are up and running. The proxy service is not necessary to run for the following exercices. 

### iv) Add data
Expand provide *orders.tar.gz* file. Also, download shell scrips *splitAndSend.original.sh* and *splitAndSend.sh* and the Python script *count-buys.py*. 
```{bash eval=FALSE}
git clone https://github.com/greenore/e63.git
cd e63

# Untar
tar xvzf orders.tar.gz 
```

#### Setup hadoop directory structure
```{bash eval=FALSE}
# Create 3 directories in hdfs: staging, splits, out/out1
hadoop fs -mkdir -p staging input output

hadoop fs -ls
```

```text
[cloudera@quickstart e63]$ hadoop fs -ls
Found 3 items
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 07:41 stating
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 07:41 input
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 07:41 output
```

### v) SplitAndSend Original
First run *splitAndSend.original.sh* and *count-buys.py*. Record the failure mode of *count-buys.py*. Simply read the error message produced and tell us what is happening.

```{bash eval=FALSE}
# Run scripts
chmod +x splitAndSend.original.sh
./splitAndSend.original.sh input
```

```{bash eval=FALSE}
spark-submit --master local[4] /home/cloudera/e63/count-buys.py
```

```text
[cloudera@quickstart e63]$ spark-submit --master local[4] /home/cloudera/e63/count-buys.py
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[Stage 1:>                                                          (0 + 4) / 4]17/10/13 10:02:09 ERROR JobScheduler: Error running job streaming job 1507888926000 ms.0
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: null
        at py4j.Protocol.getReturnValue(Protocol.java:434)
        at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:113)
        at com.sun.proxy.$Proxy17.call(Unknown Source)
        at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
        at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
        at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:189)
        at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:189)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
        at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
        at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
17/10/13 10:02:09 ERROR JobScheduler: Error generating jobs for time 1507888929000 ms
...
```

The SLF4J error can be ignored. However, if the number of standard deviations by which a task's average progress-rates is lower than the average of all running tasks the so called *race condition* is appearing. That means that by copying directly from the local file system to the hadoop *input* directory, there are cases where Spark tries to read the file just after it's name was changes. This leads to files named chunkaa._COPYING_". When the file is finished copying, the extension is changed. If Spark now sees the new temp file and then tries to read it just after its name was changed, it causes a *file not found error* and the scripts exits. That means that once one of the two tasks finishes successfully, the other task is killed.

### vi) Eliminate race condition
Then run script *splitAndSend.sh* and Python program *count-buys.py* and tell us what the results are. In both cases show use contents of your HDFS directories input, output and staging. The second script *splitAndSend.sh* is supposed to reduce or eliminate the race condition.  In both cases, show the partial contents of your HDFS directories *input*, *output* and *staging*. 

#### Empty folders
```{bash eval=FALSE}
# Empty folders
hadoop fs -rm -r staging/*
hadoop fs -rm -r output/*
hadoop fs -rm -r input/*
```

#### Run scripts
```{bash eval=FALSE}
# Run scripts
chmod +x splitAndSend.sh
./splitAndSend.sh input
```

```{bash eval=FALSE}
spark-submit --master local[4] /home/cloudera/e63/count-buys.py
```

#### Show content
```{bash eval=FALSE}
hadoop fs -ls staging
```

```text
Found 1 items
-rw-r--r--   1 cloudera cloudera     458673 2017-10-13 08:34 staging/chunkbh
```

```{bash eval=FALSE}
hadoop fs -ls input
```
```text
[cloudera@quickstart sbin]$ hadoop fs -ls splits
Found 50 items
-rw-r--r--   1 cloudera cloudera     437626 2017-10-12 12:50 splits/chunkaa
-rw-r--r--   1 cloudera cloudera     448647 2017-10-12 12:50 splits/chunkab
-rw-r--r--   1 cloudera cloudera     448605 2017-10-12 12:50 splits/chunkac
-rw-r--r--   1 cloudera cloudera     448794 2017-10-12 12:51 splits/chunkad
-rw-r--r--   1 cloudera cloudera     448624 2017-10-12 12:51 splits/chunkae
-rw-r--r--   1 cloudera cloudera     448553 2017-10-12 12:51 splits/chunkaf
-rw-r--r--   1 cloudera cloudera     448436 2017-10-12 12:51 splits/chunkag
-rw-r--r--   1 cloudera cloudera     448679 2017-10-12 12:51 splits/chunkah
-rw-r--r--   1 cloudera cloudera     448424 2017-10-12 12:51 splits/chunkai
-rw-r--r--   1 cloudera cloudera     448564 2017-10-12 12:51 splits/chunkaj
-rw-r--r--   1 cloudera cloudera     458595 2017-10-12 12:51 splits/chunkak
-rw-r--r--   1 cloudera cloudera     458580 2017-10-12 12:52 splits/chunkal
-rw-r--r--   1 cloudera cloudera     458605 2017-10-12 12:52 splits/chunkam
-rw-r--r--   1 cloudera cloudera     458630 2017-10-12 12:52 splits/chunkan
-rw-r--r--   1 cloudera cloudera     458663 2017-10-12 12:52 splits/chunkao
-rw-r--r--   1 cloudera cloudera     458540 2017-10-12 12:52 splits/chunkap
-rw-r--r--   1 cloudera cloudera     458533 2017-10-12 12:52 splits/chunkaq
-rw-r--r--   1 cloudera cloudera     458403 2017-10-12 12:52 splits/chunkar
-rw-r--r--   1 cloudera cloudera     458470 2017-10-12 12:53 splits/chunkas
-rw-r--r--   1 cloudera cloudera     458584 2017-10-12 12:53 splits/chunkat
-rw-r--r--   1 cloudera cloudera     458333 2017-10-12 12:53 splits/chunkau
-rw-r--r--   1 cloudera cloudera     458600 2017-10-12 12:53 splits/chunkav
-rw-r--r--   1 cloudera cloudera     458732 2017-10-12 12:53 splits/chunkaw
-rw-r--r--   1 cloudera cloudera     458559 2017-10-12 12:54 splits/chunkax
-rw-r--r--   1 cloudera cloudera     458654 2017-10-12 12:54 splits/chunkay
-rw-r--r--   1 cloudera cloudera     458533 2017-10-12 12:54 splits/chunkaz
-rw-r--r--   1 cloudera cloudera     458535 2017-10-12 12:54 splits/chunkba
-rw-r--r--   1 cloudera cloudera     458700 2017-10-12 12:54 splits/chunkbb
-rw-r--r--   1 cloudera cloudera     458509 2017-10-12 12:54 splits/chunkbc
-rw-r--r--   1 cloudera cloudera     458488 2017-10-12 12:54 splits/chunkbd
-rw-r--r--   1 cloudera cloudera     458527 2017-10-12 12:55 splits/chunkbe
-rw-r--r--   1 cloudera cloudera     458662 2017-10-12 12:55 splits/chunkbf
-rw-r--r--   1 cloudera cloudera     458618 2017-10-12 12:55 splits/chunkbg
-rw-r--r--   1 cloudera cloudera     458673 2017-10-12 12:55 splits/chunkbh
-rw-r--r--   1 cloudera cloudera     458431 2017-10-12 12:55 splits/chunkbi
-rw-r--r--   1 cloudera cloudera     458464 2017-10-12 12:55 splits/chunkbj
-rw-r--r--   1 cloudera cloudera     458450 2017-10-12 12:55 splits/chunkbk
-rw-r--r--   1 cloudera cloudera     458536 2017-10-12 12:56 splits/chunkbl
-rw-r--r--   1 cloudera cloudera     458412 2017-10-12 12:56 splits/chunkbm
-rw-r--r--   1 cloudera cloudera     458712 2017-10-12 12:56 splits/chunkbn
-rw-r--r--   1 cloudera cloudera     458586 2017-10-12 12:56 splits/chunkbo
-rw-r--r--   1 cloudera cloudera     458501 2017-10-12 12:56 splits/chunkbp
-rw-r--r--   1 cloudera cloudera     458569 2017-10-12 12:56 splits/chunkbq
-rw-r--r--   1 cloudera cloudera     458459 2017-10-12 12:57 splits/chunkbr
-rw-r--r--   1 cloudera cloudera     458737 2017-10-12 12:57 splits/chunkbs
-rw-r--r--   1 cloudera cloudera     458556 2017-10-12 12:57 splits/chunkbt
-rw-r--r--   1 cloudera cloudera     458358 2017-10-12 12:57 splits/chunkbu
-rw-r--r--   1 cloudera cloudera     458570 2017-10-12 12:57 splits/chunkbv
-rw-r--r--   1 cloudera cloudera     458639 2017-10-12 12:57 splits/chunkbw
-rw-r--r--   1 cloudera cloudera     458503 2017-10-12 12:57 splits/chunkbx
```

```{bash eval=FALSE}
hadoop fs -ls -R output
```

```text
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883746000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883746000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883746000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883749000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         22 2017-10-13 08:36 output/output-1507883749000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883749000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883752000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883752000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883752000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883755000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         20 2017-10-13 08:36 output/output-1507883755000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883755000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883758000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883758000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883758000.txt
-rw-r--r--   1 cloudera cloudera         21 2017-10-13 08:36 output/output-1507883761000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883761000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883761000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883764000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         20 2017-10-13 08:36 output/output-1507883764000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883764000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883767000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         21 2017-10-13 08:36 output/output-1507883767000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883767000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883777000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883777000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883777000.txt
```

#### Show part-00000
In the second run, locate an output file named *part-00000* that is not empty and show its content to us.  

```{bash eval=FALSE}
hadoop fs -cat output/output-1507883755000.txt/part-00000
```

## Problem 3 (10%)
In the second run of the previous problem you will notice that many of part-00000 files in your output directory are empty. Could you explain why.
(10%)

Some part-00000 files are empty. Those are not empty contain the result of counting of stock buys in the particular (5) second interval. The result is in the form :
(False, 4962L)
(True, 5038L)
In the examided 5 seconds there were 5038 buys and 4962 sells.

The empty files mean that there was no input in that particular interval.

## Problem 4 (20%)
Could you rewrite count-buys.sh in Spark Structured Streaming API. If you do that change script splitAndSend.sh to move generated chunks from the local files system directory staging to local file system directory input. Run this experiment on your VM with Spark 2.2.

### i) Rewrite splitAndSend
In order to use the native *Spark Structured Streaming API* we need to rewrite *splitAndSend.sh* so that it runds locally and no longer on the *HDFS* system. In order to do that first the two folder *input* and *staging* have to be created on the local filesystem. 

```{bash eval=FALSE}
### Generate folders
mkdir -p /home/tim/e63-coursework/hw6/data/input/
mkdir -p /home/tim/e63-coursework/hw6/data/staging
```

Next we can rewrite *splitAndSend.sh* to *splitAndSend2.sh*

```{bash eval=FALSE}
#!/bin/bash

# Split data into chunks
split -l 10000 /home/tim/e63-coursework/hw6/data/orders.txt /home/tim/e63-coursework/hw6/data/staging/chunk

# Simulate stream with 3 seconds difference
for f in `ls /home/tim/e63-coursework/hw6/data/staging/chunk*`; do
        sleep 3
        mv $f /home/tim/e63-coursework/hw6/data/input/
        rm -f $f
done
```

### ii) Rewrite count-buys
Next we have to rewrite the count-buys function. In principal what we have to do is to define the Schema for the input rows, create the spark session, create a DataFrame representing the stream of input files, count the buys / sells in the window using the timestamp as a watermark. The resulting file can be found in *count-buys2.py*. It's code is below:

```{bash eval=FALSE}
# Libraries
from __future__ import print_function
import findspark
findspark.init("/home/tim/spark")
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Build session
spark = SparkSession.builder.appName("StructuredCountBuys").getOrCreate()

# Define schema
schema = StructType([StructField('time', TimestampType(), True),
                     StructField('orderId', IntegerType(), True),
                     StructField('clientId', IntegerType(), True),
                     StructField('symbol', StringType(), True),
                     StructField('amount', IntegerType(), True),
                     StructField('price', FloatType(), True),
                     StructField('buy', StringType(), True)])

# Read stream
df_stockmarket = spark.readStream.csv("/home/tim/e63-coursework/hw6/data/input/",
                                      schema=schema, sep=',')

# Group by (with watermark and window)
df_buys = df_stockmarket.withWatermark("time", "1 minutes") \
                        .groupBy("buy", window("time", "1 seconds")) \
                        .count()

# Write stream
df_buys.writeStream.queryName("aggregates") \
       .outputMode("complete") \
       .format("console") \
       .start() \
       .awaitTermination()
```


### iii) Run Spark API Stream
Next we can run the code.


#### SplitandSend
```{bash eval=FALSE}
# rm -r /home/tim/e63-coursework/hw6/data/staging/*
# rm -r /home/tim/e63-coursework/hw6/data/input/*
bash /home/tim/e63-coursework/hw6/scripts/splitAndSend2.sh
```

#### Count-buys
```{bash eval=FALSE}
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/count-buys2.py
```

```text
tim@ip-172-31-24-35:~/e63-coursework/hw6/scripts$ spark-submit --master local[4] /home/tim/e63-coursewor 
k/hw6/scripts/count-buys4.py
-------------------------------------------
Batch: 0
-------------------------------------------

[Stage 0:>                                                          (0 + 3) / 3]
[Stage 1:===>                                                    (12 + 4) / 200]
[Stage 1:=====>                                                  (21 + 4) / 200]
[Stage 1:=======>                                                (26 + 4) / 200]
[Stage 1:=========>                                              (35 + 4) / 200]
[Stage 1:============>                                           (43 + 4) / 200]
[Stage 1:==============>                                         (53 + 4) / 200]
[Stage 1:=================>                                      (64 + 4) / 200]
[Stage 1:=====================>                                  (75 + 5) / 200]
[Stage 1:=======================>                                (85 + 4) / 200]
[Stage 1:===========================>                            (97 + 4) / 200]
[Stage 1:==============================>                        (110 + 4) / 200]
[Stage 1:=================================>                     (123 + 4) / 200]
[Stage 1:=====================================>                 (135 + 4) / 200]
[Stage 1:========================================>              (149 + 4) / 200]
[Stage 1:===========================================>           (159 + 4) / 200]
[Stage 1:===============================================>       (171 + 4) / 200]
[Stage 1:===================================================>   (187 + 4) / 200]
                                                                                
+---+--------------------+-----+
|buy|              window|count|
+---+--------------------+-----+
|  S|[2016-03-22 20:25...|14871|
|  B|[2016-03-22 20:25...|15129|
+---+--------------------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------

[Stage 4:=======================================>                   (2 + 1) / 3]
[Stage 5:============>                                           (46 + 4) / 200]
[Stage 5:================>                                       (59 + 4) / 200]
[Stage 5:====================>                                   (72 + 4) / 200]
[Stage 5:========================>                               (86 + 4) / 200]
[Stage 5:===========================>                           (101 + 4) / 200]
[Stage 5:===============================>                       (116 + 4) / 200]
[Stage 5:====================================>                  (134 + 4) / 200]
[Stage 5:=========================================>             (151 + 4) / 200]
[Stage 5:============================================>          (163 + 4) / 200]
[Stage 5:=================================================>     (179 + 4) / 200]
[Stage 5:=====================================================> (193 + 5) / 200]
                                                                                
+---+--------------------+-----+
|buy|              window|count|
+---+--------------------+-----+
|  S|[2016-03-22 20:25...|27556|
|  B|[2016-03-22 20:25...|28012|
|  B|[2016-03-22 20:25...| 2236|
|  S|[2016-03-22 20:25...| 2196|
+---+--------------------+-----+

-------------------------------------------
Batch: 2
-------------------------------------------

[Stage 9:==========>                                             (37 + 4) / 200]
[Stage 9:==============>                                         (50 + 4) / 200]
[Stage 9:==================>                                     (67 + 4) / 200]
[Stage 9:=======================>                                (83 + 4) / 200]
[Stage 9:===========================>                            (98 + 4) / 200]
[Stage 9:==============================>                        (110 + 4) / 200]
[Stage 9:==================================>                    (125 + 4) / 200]
[Stage 9:=======================================>               (145 + 4) / 200]
[Stage 9:============================================>          (162 + 4) / 200]
[Stage 9:=================================================>     (181 + 4) / 200]
                                                                                
+---+--------------------+-----+
|buy|              window|count|
+---+--------------------+-----+
|  S|[2016-03-22 20:25...|27556|
|  B|[2016-03-22 20:25...|28012|
|  B|[2016-03-22 20:25...| 7201|
|  S|[2016-03-22 20:25...| 7231|
+---+--------------------+-----+
```

## Problem 5 (20%)
Examine provided Python program stateful_wordcount.py. Make it work as is. If there are errors on the code, fix them. 

### i) Stateful wordcount
The script *stateful-wordcount.py* counts words in UTF8 encoded, '\n' delimited text received from the nework every second. To run the stateful_wordcount first the netcat server has to be run. 

```{bash eval=FALSE}
nc -lk 9999
```

The *stateful_wordcount.py* script has produces errors as " instead of # are used to comment the text. After fixing this, the code is running:

```{bash eval=FALSE}
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/stateful_wordcount.py localhost 9999
```

#### Netcat server
```text
tim@ip-172-31-24-35:~/e63-coursework/hw6/data$ nc -lk 9999
hello world
this is a test
talking to the world
an apple a day, keeps the doctor away
between you and me, britney is beautiful
```

#### Stateful wordcount
```text
-------------------------------------------
Time: 2017-10-13 15:50:45
-------------------------------------------
(u'this', 1)
(u'a', 2)
(u'and', 1)
(u'test', 1)
(u'beautiful', 1)
(u'apple', 1)
(u'to', 1)
(u'me,', 1)
(u'talking', 1)
(u'world', 2)
...
```

### ii) Word count (a & b)
Modify the code so that it outputs the number of words starting with letters a and b. Demonstrate that modified program work. You should provide several both positive and negative examples.

Next we're rewriting the the script. The code from the resulting *stateful_wordcount2.py* is below:

```{bash eval=FALSE}
# Load libraries
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Exit condition
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: stateful_network_wordcount.py <hostname> <port>", file=sys.stderr)
        exit(-1)
    
    # Load Sparkcontext
    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 3)
    ssc.checkpoint("checkpoint")

    # Define update function
    def updateFunc(new_values, last_sum):
        return sum(new_values) + (last_sum or 0)

    # Load data
    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    
    # Map and filter data
    running_counts = lines.flatMap(lambda line: line.split(" "))\
                          .filter(lambda x: x.startswith("a") or x.startswith("b"))\
                          .map(lambda word: (word, 1))\
                          .updateStateByKey(updateFunc)
    
    # Print data
    running_counts.pprint()

    ssc.start()
    ssc.awaitTermination()
```

As the *netcat* server is still running we only have to run the *stateful_wordcount2.py* script from above.

```{bash eval=FALSE}
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/stateful_wordcount2.py localhost 9999
```

#### Netcat server
```text
tim@ip-172-31-24-35:~/e63-coursework/hw6/data$ nc -lk 9999
hello world
this is a test
talking to the world
an apple a day, keeps the doctor away
between you and me, britney is beautiful
```

#### Stateful wordcount
```text
-------------------------------------------
Time: 2017-10-13 16:01:48
-------------------------------------------
(u'a', 2)
(u'and', 1)
(u'beautiful', 1)
(u'apple', 1)
(u'away', 1)
(u'britney', 1)
(u'an', 1)
(u'between', 1)
```

The above code shows, that all the words starting with an a or b are selected (positives). Words starting with a different letter are not selected (negatives).
