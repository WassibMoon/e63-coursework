---
title: |
  | Homework 6: Spark Streaming
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
linkcolor: blue
subtitle: |
  | E-63 Big Data Analytics
  | Harvard University, Autumn 2017
affiliation: Harvard University
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```

## Problem 1 (20%)
Lecture notes contain script *network-count.py* in both Spark Streaming APIand Spark Structured Streaming API. Use Linux *nc (NetCat)* utility to demonstrate that scripts work. Run both scripts on your own VM with Spark 2.2 installation. Cloudera VM with Spark 1.6 does not have Spark Structured Streaming API.

First we're starting spark
```{bash eval=FALSE}
sudo spark/sbin/start-all.sh
```
```text
starting org.apache.spark.deploy.master.Master, logging to /home/tim/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-ip-172-31-24-35.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/tim/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ip-172-31-24-35.out
```

Next we're running the *network-count.py* in one window and the *nc -lk 9999* and typing *abc* in another window gives the following result:
```{bash eval=FALSE}
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/network-count.py localhost 9999
nc -lk 9999 
```

```text
-------------------------------------------
Time: 2017-10-11 10:30:42
-------------------------------------------
(u'abc', 3)

-------------------------------------------
Time: 2017-10-11 10:30:45
-------------------------------------------
(u'abc', 1)

-------------------------------------------
-------------------------------------------
Time: 2017-10-11 10:30:48
-------------------------------------------
```

As can be seen above, the *network-count.py* script appears to be working.

## Problem 2 (30%)
Expand provide *orders.tar.gz* file. Also, download shell scrips *splitAndSend.original.sh* and *splitAndSend.sh* and the Python script *count-buys.py*. First run *splitAndSend.original.sh* and *count-buys.py*. Record the failure mode of *count-buys.py*. Simply read the error message produced and tell us what is happening. Then run script *splitAndSend.sh* and Python program *count-buys.py* and tell us what the results are. In both cases show use contents of your HDFS directories input, output and staging. The second script *splitAndSend.sh* is supposed to reduce or eliminate the race condition.  You might want to rename HDFS directory output from the first run in order to preserve itâ€™s content. In both cases, show the partial contents of your HDFS directories *input*, *output* and *staging*. In the second run, locate an output file named *part-00000* that is not empty and show its content to us.  Run these experiments on Cloudera VM. You need HDFS for these programs to run.

## Problem 3 (10%)
In the second run of the previous problem you will notice that many of part-00000 files in your output directory are empty. Could you explain why.
(10%)

## Problem 4 (20%)
Could you rewrite count-buys.sh in Spark Structured Streaming API. If you do that change script splitAndSend.sh to move generated chunks from the local files system directory staging to local file system directory input. Run this experiment on your VM with Spark 2.2.

## Problem 5 (20%)
Examine provided Python program stateful_wordcount.py. Make it work as is. If there are errors on the code, fix them. Modify the code so that it outputs the number of words starting with letters a and b. Demonstrate that modified program work. You should provide several both positive and negative examples.
