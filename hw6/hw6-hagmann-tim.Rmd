---
title: |
  | Homework 6: Spark Streaming
author: "Tim Hagmann"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    css: css/styles.css
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
linkcolor: blue
subtitle: |
  | E-63 Big Data Analytics
  | Harvard University, Autumn 2017
affiliation: Harvard University
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,
                      tidy.opts=list(width.cutoff=60), fig.pos='H',
                      fig.align='center')
```


## Problem 1 (20%)
Lecture notes contain script *network-count.py* in both Spark Streaming APIand Spark Structured Streaming API. Use Linux *nc (NetCat)* utility to demonstrate that scripts work. Run both scripts on your own VM with Spark 2.2 installation. Cloudera VM with Spark 1.6 does not have Spark Structured Streaming API.

First we're starting spark

```{bash eval=FALSE}
sudo spark/sbin/start-all.sh
```
```text
starting org.apache.spark.deploy.master.Master, logging to /home/tim/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-ip-172-31-24-35.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/tim/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ip-172-31-24-35.out
```

Next we're running the *network-count.py* in one window and the *nc -lk 9999* and typing *abc* in another window gives the following result:
```{bash eval=FALSE}
spark-submit --master local[4] /home/tim/e63-coursework/hw6/scripts/network-count.py localhost 9999
nc -lk 9999 
```

```text
-------------------------------------------
Time: 2017-10-11 10:30:42
-------------------------------------------
(u'abc', 3)

-------------------------------------------
Time: 2017-10-11 10:30:45
-------------------------------------------
(u'abc', 1)

-------------------------------------------
-------------------------------------------
Time: 2017-10-11 10:30:48
-------------------------------------------
```

As can be seen above, the *network-count.py* script appears to be working.

## Problem 2 (30%)
Run the following experiments on Cloudera VM. You need HDFS for these programs to run.

### WSL Setup
A lot of company notebook don't allow running virtual machines (VT-X is disabled). This is also the case in my case. An alternative of using VM is to use the docker image. After installing docker nativily, it is possible to run docker against the WSL engine. This is because Docker can expose a TCP endpoint which the CLI (i.e., WSL) can attach to.

**Note:** The TCP endpoint on Windows is turned off by default. To activate it, right-click the Docker icon in your taskbar and choose Settings, and tick the box next to "Expose daemon on tcp://localhost:2375 without TLS".

### Add docker to WSL
With that done, all we need to do is instruct the CLI under Bash to connect to the engine running under Windows instead of to the non-existing engine running under Bash, like this:

```{bash eval=FALSE}
echo "export DOCKER_HOST='tcp://0.0.0.0:2375'" >> ~/.bashrc
source ~/.bashrc
```

### Install CDH 5.12
```{bash eval=FALSE}
docker pull cloudera/quickstart:latest
```

### Start docker
```{bash eval=FALSE}
docker run --hostname=quickstart.cloudera --privileged=true -it --rm -i -t -p 8888:8888 -p 80:80 -p 7180:7180 -p 8080:8080 -p 18080:18080 -p 18081:18081 -p 8020:8020 cloudera/quickstart /usr/bin/docker-quickstart
```

### Start second docker
Starting a secon terminal it is possible to login to docker from it. In order to do this we first need the container image id.
```{bash eval=FALSE}
# Get id name
docker ps
```

```text
CONTAINER ID        IMAGE                 COMMAND                  CREATED
819a984e686f        cloudera/quickstart   "/usr/bin/docker-q..."   2 minutes ago

STATUS              PORTS
Up 2 minutes        0.0.0.0:80->80/tcp, 0.0.0.0:7180->7180/tcp, 0.0.0.0:8080->8080/tcp, ...
```

With the above information we an login to docker.
```{bash eval=FALSE}
# Login with a second screen
docker exec -it --user root 4bff53893b88 bash
```

### Change to cloudera user
The docker image starts by default into the root user. We're changing to the installed cloudera user (password=cloudera). 
```{bash eval=FALSE}
su cloudera
cd ~
```

### Setup Image
```{bash eval=FALSE}
# Add environment variables
echo "export JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera" >> /home/cloudera/.bash_profile
echo "export SPARK_HOME=/usr/lib/spark" >> /home/cloudera/.bash_profile
echo "export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce" >> /home/cloudera/.bash_profile
echo "export PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$SPARK_HOME/bin" >> /home/cloudera/.bash_profile
source /home/cloudera/.bash_profile
```

```{bash eval=FALSE}
sudo yum install python-pip
sudo pip install --upgrade pip
sudo pip install findspark
```

### Check status
```{bash eval=FALSE}
for x in `cd /etc/init.d ; ls hadoop-*`; do sudo service $x status ; done
```

```text
[root@quickstart /]# for x in `cd /etc/init.d ; ls hadoop-*`; do sudo service $x status ; done
Hadoop datanode is running                                 [  OK  ]
Hadoop journalnode is running                              [  OK  ]
Hadoop namenode is running                                 [  OK  ]
Hadoop secondarynamenode is running                        [  OK  ]
Hadoop httpfs is running                                   [  OK  ]
Hadoop historyserver is running                            [  OK  ]
Hadoop nodemanager is running                              [  OK  ]
Hadoop proxyserver is not running                          [FAILED]
Hadoop resourcemanager is running                          [  OK  ]
```
The aboe output shows, that all the necessary hadoop services are up and running. The proxy service is not necessary to run for the following exercices. 

### Add data
Expand provide *orders.tar.gz* file. Also, download shell scrips *splitAndSend.original.sh* and *splitAndSend.sh* and the Python script *count-buys.py*. 
```{bash eval=FALSE}
git clone https://github.com/greenore/e63.git
cd e63

# Untar
tar xvzf orders.tar.gz 
```

### Setup hadoop directory structure
```{bash eval=FALSE}
# Create 3 directories in hdfs: staging, splits, out/out1
hadoop fs -mkdir -p staging splits out/out1
hadoop fs -mkdir -p input output

hadoop fs -ls
```

```text
[cloudera@quickstart e63]$ hadoop fs -ls
Found 3 items
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 07:41 out
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 07:41 splits
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 07:41 staging
```

### i) Run original code
First run *splitAndSend.original.sh* and *count-buys.py*. Record the failure mode of *count-buys.py*. Simply read the error message produced and tell us what is happening.

```{bash eval=FALSE}
# Run scripts
chmod +x splitAndSend.original.sh
./splitAndSend.original.sh splits
```

```{bash eval=FALSE}
spark-submit --master local[4] /home/cloudera/e63/count-buys.py
```

```text
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/10/12 12:37:01 INFO spark.SparkContext: Running Spark version 1.6.0
17/10/12 12:37:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/10/12 12:37:02 INFO spark.SecurityManager: Changing view acls to: cloudera
17/10/12 12:37:02 INFO spark.SecurityManager: Changing modify acls to: cloudera
17/10/12 12:37:02 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
17/10/12 12:37:03 INFO util.Utils: Successfully started service 'sparkDriver' on port 45365.
17/10/12 12:37:03 INFO slf4j.Slf4jLogger: Slf4jLogger started
17/10/12 12:37:03 INFO Remoting: Starting remoting
17/10/12 12:37:04 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:37413]
17/10/12 12:37:04 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@172.17.0.2:37413]
17/10/12 12:37:04 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 37413.
17/10/12 12:37:04 INFO spark.SparkEnv: Registering MapOutputTracker
17/10/12 12:37:04 INFO spark.SparkEnv: Registering BlockManagerMaster
17/10/12 12:37:04 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-36d03cbc-76ad-44d2-b935-31e4561a3f66
17/10/12 12:37:04 INFO storage.MemoryStore: MemoryStore started with capacity 530.3 MB
17/10/12 12:37:06 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/10/12 12:37:11 INFO server.Server: jetty-8.y.z-SNAPSHOT
17/10/12 12:37:13 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
17/10/12 12:37:13 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
Killed
```

The SLF4J error can be ignored. However, if the number of standard deviations by which a task's average progress-rates is lower than the average of all running tasks the so called *race condition* is appearing. That means that once one of the two tasks finishes successfully, the other task is killed.

### ii) Eliminate race condition
Then run script *splitAndSend.sh* and Python program *count-buys.py* and tell us what the results are. In both cases show use contents of your HDFS directories input, output and staging. The second script *splitAndSend.sh* is supposed to reduce or eliminate the race condition.  You might want to rename HDFS directory output from the first run in order to preserve its content. In both cases, show the partial contents of your HDFS directories *input*, *output* and *staging*. 
 
### Empty folders
```{bash eval=FALSE}
# Empty folders
hadoop fs -rm -r staging/*
hadoop fs -rm -r splits/*
hadoop fs -rm -r out/out1/*
hadoop fs -rm -r output/*
hadoop fs -rm -r input/*
```

### Run scripts
```{bash eval=FALSE}
# Run scripts
chmod +x splitAndSend.sh
./splitAndSend.sh input
```

```{bash eval=FALSE}
spark-submit --master local[2] /home/cloudera/e63/count-buys.py
```

### Delete files
```{bash eval=FALSE}
hadoop fs -rm -r staging/* splits/* out/out1/*
```

### Show content
```{bash eval=FALSE}
hadoop fs -ls staging
```

```text
Found 1 items
-rw-r--r--   1 cloudera cloudera     458673 2017-10-13 08:34 staging/chunkbh
```

```{bash eval=FALSE}
hadoop fs -ls splits
```
```text
[cloudera@quickstart sbin]$ hadoop fs -ls splits
Found 50 items
-rw-r--r--   1 cloudera cloudera     437626 2017-10-12 12:50 splits/chunkaa
-rw-r--r--   1 cloudera cloudera     448647 2017-10-12 12:50 splits/chunkab
-rw-r--r--   1 cloudera cloudera     448605 2017-10-12 12:50 splits/chunkac
-rw-r--r--   1 cloudera cloudera     448794 2017-10-12 12:51 splits/chunkad
-rw-r--r--   1 cloudera cloudera     448624 2017-10-12 12:51 splits/chunkae
-rw-r--r--   1 cloudera cloudera     448553 2017-10-12 12:51 splits/chunkaf
-rw-r--r--   1 cloudera cloudera     448436 2017-10-12 12:51 splits/chunkag
-rw-r--r--   1 cloudera cloudera     448679 2017-10-12 12:51 splits/chunkah
-rw-r--r--   1 cloudera cloudera     448424 2017-10-12 12:51 splits/chunkai
-rw-r--r--   1 cloudera cloudera     448564 2017-10-12 12:51 splits/chunkaj
-rw-r--r--   1 cloudera cloudera     458595 2017-10-12 12:51 splits/chunkak
-rw-r--r--   1 cloudera cloudera     458580 2017-10-12 12:52 splits/chunkal
-rw-r--r--   1 cloudera cloudera     458605 2017-10-12 12:52 splits/chunkam
-rw-r--r--   1 cloudera cloudera     458630 2017-10-12 12:52 splits/chunkan
-rw-r--r--   1 cloudera cloudera     458663 2017-10-12 12:52 splits/chunkao
-rw-r--r--   1 cloudera cloudera     458540 2017-10-12 12:52 splits/chunkap
-rw-r--r--   1 cloudera cloudera     458533 2017-10-12 12:52 splits/chunkaq
-rw-r--r--   1 cloudera cloudera     458403 2017-10-12 12:52 splits/chunkar
-rw-r--r--   1 cloudera cloudera     458470 2017-10-12 12:53 splits/chunkas
-rw-r--r--   1 cloudera cloudera     458584 2017-10-12 12:53 splits/chunkat
-rw-r--r--   1 cloudera cloudera     458333 2017-10-12 12:53 splits/chunkau
-rw-r--r--   1 cloudera cloudera     458600 2017-10-12 12:53 splits/chunkav
-rw-r--r--   1 cloudera cloudera     458732 2017-10-12 12:53 splits/chunkaw
-rw-r--r--   1 cloudera cloudera     458559 2017-10-12 12:54 splits/chunkax
-rw-r--r--   1 cloudera cloudera     458654 2017-10-12 12:54 splits/chunkay
-rw-r--r--   1 cloudera cloudera     458533 2017-10-12 12:54 splits/chunkaz
-rw-r--r--   1 cloudera cloudera     458535 2017-10-12 12:54 splits/chunkba
-rw-r--r--   1 cloudera cloudera     458700 2017-10-12 12:54 splits/chunkbb
-rw-r--r--   1 cloudera cloudera     458509 2017-10-12 12:54 splits/chunkbc
-rw-r--r--   1 cloudera cloudera     458488 2017-10-12 12:54 splits/chunkbd
-rw-r--r--   1 cloudera cloudera     458527 2017-10-12 12:55 splits/chunkbe
-rw-r--r--   1 cloudera cloudera     458662 2017-10-12 12:55 splits/chunkbf
-rw-r--r--   1 cloudera cloudera     458618 2017-10-12 12:55 splits/chunkbg
-rw-r--r--   1 cloudera cloudera     458673 2017-10-12 12:55 splits/chunkbh
-rw-r--r--   1 cloudera cloudera     458431 2017-10-12 12:55 splits/chunkbi
-rw-r--r--   1 cloudera cloudera     458464 2017-10-12 12:55 splits/chunkbj
-rw-r--r--   1 cloudera cloudera     458450 2017-10-12 12:55 splits/chunkbk
-rw-r--r--   1 cloudera cloudera     458536 2017-10-12 12:56 splits/chunkbl
-rw-r--r--   1 cloudera cloudera     458412 2017-10-12 12:56 splits/chunkbm
-rw-r--r--   1 cloudera cloudera     458712 2017-10-12 12:56 splits/chunkbn
-rw-r--r--   1 cloudera cloudera     458586 2017-10-12 12:56 splits/chunkbo
-rw-r--r--   1 cloudera cloudera     458501 2017-10-12 12:56 splits/chunkbp
-rw-r--r--   1 cloudera cloudera     458569 2017-10-12 12:56 splits/chunkbq
-rw-r--r--   1 cloudera cloudera     458459 2017-10-12 12:57 splits/chunkbr
-rw-r--r--   1 cloudera cloudera     458737 2017-10-12 12:57 splits/chunkbs
-rw-r--r--   1 cloudera cloudera     458556 2017-10-12 12:57 splits/chunkbt
-rw-r--r--   1 cloudera cloudera     458358 2017-10-12 12:57 splits/chunkbu
-rw-r--r--   1 cloudera cloudera     458570 2017-10-12 12:57 splits/chunkbv
-rw-r--r--   1 cloudera cloudera     458639 2017-10-12 12:57 splits/chunkbw
-rw-r--r--   1 cloudera cloudera     458503 2017-10-12 12:57 splits/chunkbx
```

```{bash eval=FALSE}
hadoop fs -ls -R output
```

```text
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883746000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883746000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883746000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883749000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         22 2017-10-13 08:36 output/output-1507883749000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883749000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883752000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883752000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883752000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883755000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         20 2017-10-13 08:36 output/output-1507883755000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883755000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883758000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883758000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883758000.txt
-rw-r--r--   1 cloudera cloudera         21 2017-10-13 08:36 output/output-1507883761000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883761000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883761000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883764000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         20 2017-10-13 08:36 output/output-1507883764000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883764000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883767000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera         21 2017-10-13 08:36 output/output-1507883767000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883767000.txt
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883777000.txt/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2017-10-13 08:36 output/output-1507883777000.txt/part-00000
drwxr-xr-x   - cloudera cloudera          0 2017-10-13 08:36 output/output-1507883777000.txt
```

In the second run, locate an output file named *part-00000* that is not empty and show its content to us.  

```{bash eval=FALSE}
hadoop fs -cat output/output-1507883755000.txt/part-00000
```

## Problem 3 (10%)
In the second run of the previous problem you will notice that many of part-00000 files in your output directory are empty. Could you explain why.
(10%)

## Problem 4 (20%)
Could you rewrite count-buys.sh in Spark Structured Streaming API. If you do that change script splitAndSend.sh to move generated chunks from the local files system directory staging to local file system directory input. Run this experiment on your VM with Spark 2.2.

```{bash eval=FALSE}
bash scripts/splitAndSend.sh data/input local
```

## Problem 5 (20%)
Examine provided Python program stateful_wordcount.py. Make it work as is. If there are errors on the code, fix them. Modify the code so that it outputs the number of words starting with letters a and b. Demonstrate that modified program work. You should provide several both positive and negative examples.
